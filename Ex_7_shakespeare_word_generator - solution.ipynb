{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enFHuMx1n5pQ"
   },
   "source": [
    "# Downloading shakespeare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vMGJsv_tW6P"
   },
   "source": [
    "Running the cell below will create a folder called 'data' in the session storage, and store all of shakespeare's work in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1279,
     "status": "ok",
     "timestamp": 1640251755663,
     "user": {
      "displayName": "Nathan Cornille",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgISwQnk3dZ8MJS9FmcTdQOeSU7tE65qiOTpw4o=s64",
      "userId": "07989792410712645375"
     },
     "user_tz": -60
    },
    "id": "YlOKWWIatONI",
    "outputId": "af5b350f-577c-43a1-c8f9-dc0211e447e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory \"data\" has files ['shakespeare_raw.txt', 'shakespeare_sonnets.txt', 'shakespeare_plays.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import codecs\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)   \n",
    "\n",
    "shakespeare_url = 'http://www.gutenberg.org/files/100/100-0.txt'\n",
    "raw_shakespeare_data = os.path.join(DATA_DIR, 'shakespeare_raw.txt')\n",
    "if not os.path.exists(raw_shakespeare_data):\n",
    "    print('Downloading shakespeare corpus from %s' % shakespeare_url)\n",
    "    urllib.request.urlretrieve(shakespeare_url, raw_shakespeare_data)\n",
    "\n",
    "with codecs.open(raw_shakespeare_data, 'rb', 'utf-8') as f:\n",
    "    shakespeare = f.readlines()\n",
    "\n",
    "shakespeare = shakespeare[139:147417]\n",
    "\n",
    "\n",
    "boilerplate = {u'<<THIS ELECTRONIC VERSION OF THE COMPLETE WORKS OF WILLIAM\\r\\n',\n",
    "u'SHAKESPEARE IS COPYRIGHT 1990-1993 BY WORLD LIBRARY, INC., AND IS\\r\\n',\n",
    "u'PROVIDED BY PROJECT GUTENBERG ETEXT OF ILLINOIS BENEDICTINE COLLEGE\\r\\n',\n",
    "u'WITH PERMISSION.  ELECTRONIC AND MACHINE READABLE COPIES MAY BE\\r\\n',\n",
    "u'DISTRIBUTED SO LONG AS SUCH COPIES (1) ARE FOR YOUR OR OTHERS\\r\\n',\n",
    "u'PERSONAL USE ONLY, AND (2) ARE NOT DISTRIBUTED OR USED\\r\\n',\n",
    "u'COMMERCIALLY.  PROHIBITED COMMERCIAL DISTRIBUTION INCLUDES BY ANY\\r\\n',\n",
    "u'SERVICE THAT CHARGES FOR DOWNLOAD TIME OR FOR MEMBERSHIP.>>\\r\\n'}\n",
    "shakespeare = [l for l in shakespeare if l not in boilerplate]\n",
    "\n",
    "\n",
    "shakespeare_sonnets = os.path.join(DATA_DIR, 'shakespeare_sonnets.txt')\n",
    "if not os.path.exists(shakespeare_sonnets):\n",
    "    with codecs.open(shakespeare_sonnets, 'wb', 'utf-8') as f:\n",
    "        print('Saving shakespeare sonnets file.')\n",
    "        for i in range(2772):\n",
    "            f.write(shakespeare[i])\n",
    "\n",
    "\n",
    "shakespeare_plays = os.path.join(DATA_DIR, 'shakespeare_plays.txt')\n",
    "if not os.path.exists(shakespeare_plays):\n",
    "    with codecs.open(shakespeare_plays, 'wb', 'utf-8') as f:\n",
    "        print('Saving shakespeare plays file.')\n",
    "        for i in range(2773, len(shakespeare)):\n",
    "            f.write(shakespeare[i])\n",
    "print(f'Data directory \\\"{DATA_DIR}\\\" has files {os.listdir(DATA_DIR)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUxzmjITtjFM"
   },
   "source": [
    "Let's see what the first 10 lines of the plays and sonnets look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1640251755664,
     "user": {
      "displayName": "Nathan Cornille",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgISwQnk3dZ8MJS9FmcTdQOeSU7tE65qiOTpw4o=s64",
      "userId": "07989792410712645375"
     },
     "user_tz": -60
    },
    "id": "U_ZEjjc8pTPC",
    "outputId": "26d712b3-37ba-4d2c-ed26-6a56518adfb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACT I\n",
      "Scene I. Rossillon. A room in the Countess’s palace.\n",
      "Scene II. Paris. A room in the King’s palace.\n",
      "Scene III. Rossillon. A Room in the Palace.\n",
      "\n",
      "\n",
      "ACT II\n",
      "Scene I. Paris. A room in the King’s palace.\n",
      "Scene II. Rossillon. A room in the Countess’s palace.\n",
      "Scene III. Paris. The King’s palace.\n",
      "\n",
      "Making a famine where abundance lies,\n",
      "Thy self thy foe, to thy sweet self too cruel:\n",
      "Thou that art now the world’s fresh ornament,\n",
      "And only herald to the gaudy spring,\n",
      "Within thine own bud buriest thy content,\n",
      "And, tender churl, mak’st waste in niggarding:\n",
      "  Pity the world, or else this glutton be,\n",
      "  To eat the world’s due, by the grave and thee.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in ['shakespeare_plays','shakespeare_sonnets']:\n",
    "  with open(f'{DATA_DIR}/{text}.txt','r', encoding=\"utf8\") as f:\n",
    "    print(\"\".join(next(f) for _ in range(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEDlXk4egkvQ"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUCAoPJpwwey"
   },
   "source": [
    "We have the data now, but it isn't yet in a form usable by a machine learning model.\n",
    "We'll want to do:\n",
    "\n",
    "\n",
    "* Tokenization: chopping the text up into individual units. Here, those units will be words\n",
    "* Vectorization: replacing each token with an index (that will index into an emebdding matrix).\n",
    "* Creating training examples: dividing the list of tokens into sequences of some limited length, with inputs and target\n",
    "* Mini-batching: rather than waiting to perform the gradient update after the model sees _all_ the data, you can speed up learning by doing gradient updates after only a few samples.\n",
    "\n",
    "We can organize the first three of these steps in a DataSet class: this will be the interface that goes from text file to vectorized examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7548,
     "status": "ok",
     "timestamp": 1640251943532,
     "user": {
      "displayName": "Nathan Cornille",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgISwQnk3dZ8MJS9FmcTdQOeSU7tE65qiOTpw4o=s64",
      "userId": "07989792410712645375"
     },
     "user_tz": -60
    },
    "id": "1KKUyScN2S4V",
    "outputId": "d6fefd8f-1197-45dd-c0a5-618f51d312e6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\natha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "import operator\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "UNK_TOKEN = 'UNKNOWN'\n",
    "\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, corpus_file, seq_length,word_level=True,max_vocab_size=10000):\n",
    "        self.corpus_file = corpus_file\n",
    "        self.seq_length = seq_length\n",
    "        self.word_level = word_level\n",
    "        self.max_vocab_size=max_vocab_size\n",
    "\n",
    "        self.token_ids = []\n",
    "        self.token2id = {}\n",
    "        self.read_and_load_data_into_mem()\n",
    "\n",
    "        self.vocab = Vocab(self.token2id)\n",
    "\n",
    "    # Because the datasets are relatively small, we can just load them into memory\n",
    "    def read_and_load_data_into_mem(self):\n",
    "        if len(self.token_ids) == 0:\n",
    "            print(f\"Loading data from {self.corpus_file} into memory\")\n",
    "            temp_tokens = []\n",
    "            token_counts = {}\n",
    "            with open(self.corpus_file,'r',encoding='utf-8') as f:\n",
    "                for line in tqdm(f):\n",
    "                    # Here, we chunk the continuous text into tokens\n",
    "                    # First, do word_level, you can use word_tokenize from nltk. Don't forget to include '\\n' tokens though!\n",
    "                    # At the end, come back and include the option for character-level.\n",
    "                    # TODO\n",
    "                    token_list = word_tokenize(line) + ['\\n'] if self.word_level else list(line)\n",
    "                    for t in token_list:\n",
    "                        token = t.lower() if self.word_level else t\n",
    "                        if token in token_counts:\n",
    "                            token_counts[token] += 1\n",
    "                        else:\n",
    "                            token_counts[token] = 1\n",
    "                        temp_tokens.append(token)\n",
    "\n",
    "            # Here, we make self.token2id, that maps the most-appearing words to ids, based on token_counts and self.max_vocab_size\n",
    "            # TODO. Don't forget to add an extra 'common token': the UNK_TOKEN\n",
    "            token_counts = sorted(token_counts.items(), key=operator.itemgetter(1), reverse=True)  # sort by key\n",
    "            if len(token_counts) > self.max_vocab_size:\n",
    "                token_counts = token_counts[:self.max_vocab_size]\n",
    "                print('Truncating vocab.')\n",
    "            most_common_tokens = ['<unk>']\n",
    "            for t, _ in token_counts:\n",
    "                most_common_tokens.append(t)\n",
    "            self.token2id = {w: i for i, w in enumerate(most_common_tokens)}\n",
    "\n",
    "            # Finally, using this vocabulary, we can convert the word tokens into indexes, and store those.\n",
    "            self.token_ids = [self.token2id[t] if t in self.token2id else self.token2id['<unk>'] for t in temp_tokens]\n",
    "\n",
    "    def __len__(self):\n",
    "        #In this class, we want to give the number of examples. This is not the same as the number of tokens!\n",
    "        num_tokens = len(self.token_ids)\n",
    "        num_examples = (num_tokens-1) // self.seq_length # extra word for the target\n",
    "        return num_examples\n",
    "\n",
    "    def __getitem__(self, example_idx):\n",
    "        start_token_idx = example_idx*self.seq_length\n",
    "        end_token_idx = (example_idx+1)*self.seq_length\n",
    "        inputs = self.token_ids[start_token_idx:end_token_idx]\n",
    "        targets = self.token_ids[start_token_idx+1:end_token_idx+1]\n",
    "\n",
    "        # Some technicalities: to work well with pytorch, the integers need to be represented in the right format\n",
    "        tensor_inputs = torch.tensor(inputs).long()\n",
    "        tensor_targets = torch.tensor(targets).long()\n",
    "\n",
    "        return tensor_inputs,tensor_targets\n",
    "\n",
    "\n",
    "# Helper class to easily convert from tokens to indices and back\n",
    "class Vocab():\n",
    "\n",
    "  def __init__(self,token2id):\n",
    "    self.token2id = token2id\n",
    "    self.id2token = {id:word for word,id in self.token2id.items()}\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "naHCGktK2S4V",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's check out the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20419,
     "status": "ok",
     "timestamp": 1640251966235,
     "user": {
      "displayName": "Nathan Cornille",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgISwQnk3dZ8MJS9FmcTdQOeSU7tE65qiOTpw4o=s64",
      "userId": "07989792410712645375"
     },
     "user_tz": -60
    },
    "id": "U4fMn0TSvenr",
    "outputId": "b42da430-0d4b-4366-a214-fb8b05e33182"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "844it [00:00, 8435.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data\\shakespeare_plays.txt into memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "144505it [00:22, 6350.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncating vocab.\n",
      "37120\n",
      "tensor([ 269,    6,    1,  117,  732, 2229,    3,   11,  367,   14,    4,  733,\n",
      "          10,   37,  461,    3,    1,  117,  360,    3,  624,    3,   11,  367,\n",
      "          14,    4,   53,   10,   37,  461,    3,    1])\n",
      "['act', 'i', '\\n', 'scene', 'i.', 'rossillon', '.', 'a', 'room', 'in', 'the', 'countess', '’', 's', 'palace', '.', '\\n', 'scene', 'ii', '.', 'paris', '.', 'a', 'room', 'in', 'the', 'king', '’', 's', 'palace', '.', '\\n']\n"
     ]
    }
   ],
   "source": [
    "corpus = shakespeare_plays\n",
    "ds = ShakespeareDataset(corpus,seq_length=32)\n",
    "v = ds.vocab\n",
    "print(len(ds))\n",
    "example_input_idxs, example_target_idxs = ds[0]\n",
    "print(example_input_idxs)\n",
    "print([v.id2token[int(idx)] for idx in example_input_idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YpkM43cgkvS"
   },
   "source": [
    "# Model specification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8pmYc0VYkbx"
   },
   "source": [
    "In the code comments we'll use \n",
    "\n",
    "* B to denote minibatch size\n",
    "* L to denote number of timesteps / sequence length\n",
    "* V to denote the vocabulary size = dimension of the one-hot embeddings\n",
    "* E to denote (dense) embedding dimension\n",
    "* H to denote the LSTM hidden dimension\n",
    "* N to denote the number of LSTM layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 14781,
     "status": "ok",
     "timestamp": 1640251980995,
     "user": {
      "displayName": "Nathan Cornille",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgISwQnk3dZ8MJS9FmcTdQOeSU7tE65qiOTpw4o=s64",
      "userId": "07989792410712645375"
     },
     "user_tz": -60
    },
    "id": "HEbz98fPaQR9"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryq0XRXD2S4W",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We'll use the pytorch lightning framework to organize our code: this includes creating the logic for training steps, optimization, validation etc. inside your model class. The pytorch lightning library then makes sure to call these at the right time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1040,
     "status": "ok",
     "timestamp": 1640252188386,
     "user": {
      "displayName": "Nathan Cornille",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgISwQnk3dZ8MJS9FmcTdQOeSU7tE65qiOTpw4o=s64",
      "userId": "07989792410712645375"
     },
     "user_tz": -60
    },
    "id": "bl_pJTmkDi9a"
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class TimeDistributed(nn.Module):\n",
    "    def __init__(self, module, batch_first=False):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "\n",
    "        # Squash B and L into a single axis\n",
    "        x_reshape = x.contiguous().view(-1, x.size(-1))  # [B * L, H]\n",
    "\n",
    "        y = self.module(x_reshape)\n",
    "\n",
    "        # We have to reshape Y\n",
    "        if self.batch_first:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(-1))  # [B, L, V]\n",
    "        else:\n",
    "            y = y.view(-1, x.size(1), y.size(-1))  # [L, B, V]\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class GeneratorLSTM(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedder = nn.Embedding(vocab_size, emb_dim)\n",
    "        num_layers = 2\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim,num_layers,batch_first=True)\n",
    "        self.out = TimeDistributed(nn.Linear(hidden_dim, vocab_size))\n",
    "        self.hidden = None\n",
    "\n",
    "    # Hidden is a tuple that contains cell state and hiddens state\n",
    "    def forward(self, inputs,remember_hidden=False):\n",
    "        embeddings = self.embedder(inputs) # step 1: embedder: [B,L] -> [B,L,E]\n",
    "        \n",
    "        # step 2: lstm: if self.hidden is None: [B,L,E] -> [B,L,E], ([B,N,H],[B,N,H]). Else: [B,L,E],([B,N,H],[B,N,H]) -> [B,L,H], ([B,N,H],[B,N,H])\n",
    "        if self.hidden is None:\n",
    "            args = [embeddings]\n",
    "        else:\n",
    "            args = [embeddings,self.hidden]\n",
    "        lstm_hidden_states, new_hidden = self.lstm(*args)\n",
    "        \n",
    "        # step 3: out:  # [B,L,H] -> [B,L,V]        \n",
    "        vocab_scores = self.out(lstm_hidden_states)\n",
    "        if remember_hidden:\n",
    "            self.hidden = tuple(el.detach() for el in new_hidden)\n",
    "        return vocab_scores\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        inputs, targets = train_batch\n",
    "        vocab_scores = self(inputs,remember_hidden=True)  #\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        B, L, V = vocab_scores.shape\n",
    "        loss = loss_fn(vocab_scores.view(B * L, V),\n",
    "                       targets.view(B * L))  # calculate the loss in parallel for all B*L words\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    # We can use performance on a held-out validation set to get a feeling for when we are overfitting on the training data.\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        inputs, targets = val_batch\n",
    "        vocab_scores = self(inputs,remember_hidden=True)  #\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        B, L, V = vocab_scores.shape\n",
    "        loss = loss_fn(vocab_scores.view(B * L, V),\n",
    "                       targets.view(B * L))  # calculate the loss in parallel for all B*L words\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "    # The optimizer doesn't influence the gradient. However, a good optimizer makes good decisions about the learning rate for different parameters: sometimes it's better to take big steps, sometimes small steps are better.\n",
    "    # The Adam optimizer is a very good default\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def on_epoch_end(self) -> None:\n",
    "        self.hidden = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGON9dKi2S4X",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's see what an untrained model predicts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 198,
     "status": "ok",
     "timestamp": 1640252194480,
     "user": {
      "displayName": "Nathan Cornille",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgISwQnk3dZ8MJS9FmcTdQOeSU7tE65qiOTpw4o=s64",
      "userId": "07989792410712645375"
     },
     "user_tz": -60
    },
    "id": "VGvodR2O2S4X",
    "outputId": "6178b716-6592-422b-97af-d9a90d9e9fc0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT:  act i \n",
      " scene i. rossillon . a room in the countess ’ s palace . \n",
      " scene ii . paris . a room in the king ’ s palace . \n",
      "\n",
      "PREDICTED NEXT WORDS:  chop shroud shroud hole shroud hole wart wart wart shroud shroud shroud shroud stubbornness shroud wart wart wart wart wart wart wart wart wart shroud shroud shroud shroud shroud shroud wart wart\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 128\n",
    "emb_dim = 50\n",
    "vocab_size = len(ds.token2id)\n",
    "\n",
    "model = GeneratorLSTM(vocab_size,emb_dim,hidden_dim)\n",
    "sample_input,_ = ds[0]# Add dummy batch dimension\n",
    "output_scores = model(sample_input.unsqueeze(0) )\n",
    "\n",
    "print(\"INPUT: \",\" \".join(v.id2token[idx] for idx in sample_input.tolist()))\n",
    "print(\"PREDICTED NEXT WORDS: \", \" \".join(v.id2token[idx] for idx in output_scores.argmax(dim=-1)[0].tolist()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4YccxBx2S4X",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We want our model to learn on long consecutive sequences of text.\n",
    "To speed things up, we'll also want to parallelize: work in minibatches of text at the same time, after each of which a learning step is performed.\n",
    "We want to make sure the batches don't interfere with having long consecutive sequences.\n",
    "Hence, let's make a custom sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 213,
     "status": "ok",
     "timestamp": 1640252349576,
     "user": {
      "displayName": "Nathan Cornille",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgISwQnk3dZ8MJS9FmcTdQOeSU7tE65qiOTpw4o=s64",
      "userId": "07989792410712645375"
     },
     "user_tz": -60
    },
    "id": "DZQ_ikuh2S4Y",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler\n",
    "\n",
    "class CustomBatchSampler(Sampler):\n",
    "\n",
    "    # If we had 6 sequences in our dataset: 0 2 3 4 5, and we want batch size 2, we want our batch inputs to look like:\n",
    "    # [[0, 3], [1, 4], [2, 5]]. So in the first batch: 0 and 3, second batch: 1 and 4, and so on.\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset = dataset\n",
    "        self.per_batchslice_dataset_len = len(self.dataset) // self.batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter([i*self.per_batchslice_dataset_len+j for i in range(self.batch_size)] for j in range(self.per_batchslice_dataset_len))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) // self.batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IHxotIQ2S4Y",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "... not great yet. Let's train it!\n",
    "First, lets split our data into a training dataset, and a validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 213,
     "status": "ok",
     "timestamp": 1640252368792,
     "user": {
      "displayName": "Nathan Cornille",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgISwQnk3dZ8MJS9FmcTdQOeSU7tE65qiOTpw4o=s64",
      "userId": "07989792410712645375"
     },
     "user_tz": -60
    },
    "id": "JYPexkzT2S4Y",
    "outputId": "2cc54a48-100a-4069-fb63-cc82bc5b745a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35264 1856\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size=32\n",
    "cutoff = math.floor(.95*len(ds))\n",
    "training_ds, validation_ds = torch.utils.data.random_split(ds,lengths=[cutoff,len(ds)-cutoff])\n",
    "print(len(training_ds),len(validation_ds))\n",
    "train_loader = DataLoader(training_ds,batch_sampler=CustomBatchSampler(training_ds, batch_size))\n",
    "val_loader = DataLoader(validation_ds,batch_sampler=CustomBatchSampler(validation_ds, batch_size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBgklQ-h2S4Y",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Then, training is simply a matter of creating a trainer object, and calling fit on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381,
     "referenced_widgets": [
      "fae9f11a62144b6793279a736f04a259",
      "c379c4e8fed247d2b238b1f62b50bbae",
      "4c584691d1364f719010a37bd1571535",
      "747cac49b92840b9814cb34b67e577c1",
      "7b71954cf233487a800359c4e07d3a85",
      "429c13612d564925837b23e04ea5f38a",
      "922af2f87cc74257868c507bab3ceb49",
      "8413f90beaf34d1984dcb53d5f07da67",
      "c5df7f27e9614929bf3e3aef7280f5ae",
      "c142e8bc510d455dbb856e347c43002d",
      "8203e36b860f4ec1b2f29460fa2e769b",
      "9383381c89e24eaa879bb2e7ceaaddaf",
      "f6bd10c492bf4ce2ad0f4c5ae3efc85b",
      "bc782d506310438b89d79d2fc77042c2",
      "19a55abb050847298658a4283375005f",
      "8efab274a37448799a998d884c82ee83",
      "9b6e5626cf0d4cd3af6ecf9373221053",
      "0dc011835aca469d9de2b70f7dfc0547",
      "c9752f70fb9849948585c5684d0de818",
      "ba2264561be0481496ced94b317c1e50",
      "b3056923f49e40ff844b4cffd51d4f63",
      "39231b40984143b7b0dd9ed3e2c4961d",
      "e53b960fd7c84783950d73bac29c8ba9",
      "7ac4f29341fe4dd59abaf7e5e9ff3c15",
      "752b1fd4196a4947ac30b277687e651a",
      "ad4c364ea5774625a6b26ba6ffe41777",
      "98a691a38dd8477985e5234e360200f9",
      "a8a32c489f704efb9c1c2087646c114a",
      "de9aa5adb43d4d389e9dbd7d77b36c01",
      "c9f7a2c3f1b841259ef860f3746983aa",
      "a1449ea4e3e34c61b34667442e02a8e4",
      "f3c144a9e9814525904ae136026171d6",
      "c435bb03a6ab4ec8a40d4276732e6b0d"
     ]
    },
    "executionInfo": {
     "elapsed": 36645,
     "status": "ok",
     "timestamp": 1640252410401,
     "user": {
      "displayName": "Nathan Cornille",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgISwQnk3dZ8MJS9FmcTdQOeSU7tE65qiOTpw4o=s64",
      "userId": "07989792410712645375"
     },
     "user_tz": -60
    },
    "id": "s_Ijz8pQaiHc",
    "outputId": "ffcc782b-a7a7-47c5-bb34-f5bc5ad22460"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name     | Type            | Params\n",
      "---------------------------------------------\n",
      "0 | embedder | Embedding       | 500 K \n",
      "1 | lstm     | LSTM            | 224 K \n",
      "2 | out      | TimeDistributed | 1.3 M \n",
      "---------------------------------------------\n",
      "2.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 M     Total params\n",
      "8.058     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b81a613207d4018818eb05c16903985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\natha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:116: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\users\\natha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:116: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34936439e71a4ff9bcccda3ec45f54f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time limit reached. Elapsed time is 0:00:30. Signaling Trainer to stop.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df4f25b53c234e54b146b03bbb1d9e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import math\n",
    "\n",
    "batch_size=32\n",
    "trainer = pl.Trainer(max_time={\"minutes\": 0.5},callbacks=[EarlyStopping(monitor=\"val_loss\")]) # Max minutes of training\n",
    "trainer.fit(model, train_loader,val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70BAVRSP2S4Z",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "With our trained model, let's try to generate some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 684,
     "status": "ok",
     "timestamp": 1640252528870,
     "user": {
      "displayName": "Nathan Cornille",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgISwQnk3dZ8MJS9FmcTdQOeSU7tE65qiOTpw4o=s64",
      "userId": "07989792410712645375"
     },
     "user_tz": -60
    },
    "id": "bEood_9p2S4Z",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_text(model, seed_tokens, temperature, vocab, N):\n",
    "    # first we initialize the state of the LSTM using the seed_str\n",
    "    seed_token_ids = torch.tensor([vocab.token2id[w] for w in seed_tokens])\n",
    "    probs = model(seed_token_ids.unsqueeze(0),remember_hidden=True)\n",
    "\n",
    "    # now we start generating text\n",
    "    next_token_probs = probs[0,-1,:]\n",
    "    next_token_idx = sample(next_token_probs, temperature)\n",
    "    generated_text_idx = [next_token_idx]\n",
    "    generated_text = [vocab.id2token[next_token_idx]]\n",
    "    for i in range(N - 1):\n",
    "        probs = model(torch.tensor(next_token_idx).unsqueeze(0).unsqueeze(0),remember_hidden=True) # Unsqueeze for batch size and sequence length\n",
    "        next_token_probs = probs[0,-1,:]\n",
    "        next_token_idx = sample(next_token_probs, temperature)\n",
    "        generated_text_idx.append(next_token_idx)\n",
    "        generated_text.append(vocab.id2token[next_token_idx])\n",
    "    return generated_text\n",
    "\n",
    "def sample(token_probs, temperature=1.0):\n",
    "    temped_probs = token_probs / temperature\n",
    "    softmaxed_probs = F.softmax(temped_probs, dim=0)\n",
    "    sampled_idx = torch.multinomial(softmaxed_probs, 1)\n",
    "    return int(sampled_idx)\n",
    "\n",
    "\n",
    "from nltk import word_tokenize\n",
    "seed_str = '''To be or not to be'''\n",
    "seed_str = [w.lower() for w in word_tokenize(seed_str)]\n",
    "generated_text = generate_text(model, seed_str, 1.1, ds.vocab, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1640252530885,
     "user": {
      "displayName": "Nathan Cornille",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgISwQnk3dZ8MJS9FmcTdQOeSU7tE65qiOTpw4o=s64",
      "userId": "07989792410712645375"
     },
     "user_tz": -60
    },
    "id": "TGoR1_jOgkvV",
    "outputId": "612215e9-e033-41a7-8714-84c9cb39c1ad",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to be cried daughter him stout \n",
      " . and \n",
      " \n",
      " you mowbray have she \n",
      " by ajax withal leader \n",
      " knocking good was can ? lord th having april , hang , so seek turn the out and noble spirits bold king garland \n",
      " norfolk nothing up thou that such thee mother . pyramus first \n",
      " macbeth build business \n",
      " dare ladies , , gossips hear trot naked their mine heard employ \n",
      " ! all ! angel just of prove can \n",
      " ! sound thoughts therefore away his caius be . cold pity the \n",
      " it other and winchester strange my . office us are days virgilia last i comes woman ; he \n",
      " curtis dare liege volumnia \n",
      " even my servant with though timon few stern . duke count each \n",
      " , , thee spies be unless , know make more take world snow ; for can \n",
      " how palace day in \n",
      " so ! say <unk> priam any a of either . should babbling \n",
      " durst \n",
      " doubt sicken those . \n",
      " and nay chamber were ; \n",
      " ? , , extended off , \n",
      " to sampson <unk> merry brutus true fortress what night \n",
      " doth \n",
      " day face i ay call benedick their , breath no ] pucelle for thou warwick shall for she you , ] <unk> lorenzo , ? out thank under find bastard fool why good us of \n",
      " things \n",
      " trip i way that had hard trouble right rise too our haste the hastings favour is knows vouchsafe ; that tame case <unk> poor \n",
      " eggs state the , \n",
      " along countercheck \n",
      " to art dromio : them- this fine what imogen grumio \n",
      " , welcome meet ; prospero have question so \n",
      " first not , \n",
      " , \n",
      " me should three \n",
      " party\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(seed_str + generated_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tY6jkaznNnOB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally, go back and include the option to work at the character level in the ShakespeareDataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6129it [00:00, 60691.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data\\shakespeare_plays.txt into memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "144505it [00:02, 62638.07it/s]\n"
     ]
    }
   ],
   "source": [
    "c_ds = ShakespeareDataset(corpus,seq_length=64,word_level=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hidden_dim = 128\n",
    "emb_dim = 50\n",
    "vocab_size = len(c_ds.token2id)\n",
    "\n",
    "c_model = GeneratorLSTM(vocab_size,emb_dim,hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name     | Type            | Params\n",
      "---------------------------------------------\n",
      "0 | embedder | Embedding       | 5.0 K \n",
      "1 | lstm     | LSTM            | 224 K \n",
      "2 | out      | TimeDistributed | 13.0 K\n",
      "---------------------------------------------\n",
      "242 K     Trainable params\n",
      "0         Non-trainable params\n",
      "242 K     Total params\n",
      "0.969     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01386c03820e4b458cb00ad1b3b59d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\natha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:116: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71effda8f29e4cf5a6da303897c6b4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time limit reached. Elapsed time is 0:05:00. Signaling Trainer to stop.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9df98ae5fd442a991709e73cf7a09f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cutoff = math.floor(.95*len(c_ds))\n",
    "training_ds, validation_ds = torch.utils.data.random_split(c_ds,lengths=[cutoff,len(c_ds)-cutoff])\n",
    "train_loader = DataLoader(training_ds,batch_sampler=CustomBatchSampler(training_ds,batch_size))\n",
    "val_loader = DataLoader(validation_ds,batch_sampler=CustomBatchSampler(validation_ds,batch_size))\n",
    "batch_size=32\n",
    "trainer = pl.Trainer(max_time={\"minutes\": 5},callbacks=[EarlyStopping(monitor=\"val_loss\")]) # Max minutes of training\n",
    "trainer.fit(c_model, train_loader,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to bestint treatior,s are bre’fumallfor. Alord, in theich was celions tle his man’lf:\n",
      "Shukic?\n",
      "\n",
      "DER THIY. Cowesy wealded be sladI.\n",
      ":\n",
      "Thom; as sroutun hiel outs not mounk them the when’s; ald for’d maes an thee, Ancuincle, yor wlule uparcer-sary’G Torat iffelg, le thut it]Cen; by “ings cosciul monds a book\n"
     ]
    }
   ],
   "source": [
    "seed_str = '''To be or not to be'''\n",
    "seed_str = [w for w in seed_str]\n",
    "generated_text = generate_text(c_model, seed_str, 1.1, c_ds.vocab, 300)\n",
    "print(''.join(seed_str + generated_text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Ex_7_shakespeare_word_generator.ipynb",
   "provenance": [
    {
     "file_id": "17nmNyK3U7akjD7uAw5Bidv7H2D0rCjxc",
     "timestamp": 1639479125623
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0dc011835aca469d9de2b70f7dfc0547": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "19a55abb050847298658a4283375005f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ba2264561be0481496ced94b317c1e50",
      "max": 1159,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c9752f70fb9849948585c5684d0de818",
      "value": 180
     }
    },
    "39231b40984143b7b0dd9ed3e2c4961d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "429c13612d564925837b23e04ea5f38a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4c584691d1364f719010a37bd1571535": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_922af2f87cc74257868c507bab3ceb49",
      "placeholder": "​",
      "style": "IPY_MODEL_429c13612d564925837b23e04ea5f38a",
      "value": "Validation sanity check: 100%"
     }
    },
    "747cac49b92840b9814cb34b67e577c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c5df7f27e9614929bf3e3aef7280f5ae",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8413f90beaf34d1984dcb53d5f07da67",
      "value": 2
     }
    },
    "752b1fd4196a4947ac30b277687e651a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_de9aa5adb43d4d389e9dbd7d77b36c01",
      "placeholder": "​",
      "style": "IPY_MODEL_a8a32c489f704efb9c1c2087646c114a",
      "value": "Validating: 100%"
     }
    },
    "7ac4f29341fe4dd59abaf7e5e9ff3c15": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "7b71954cf233487a800359c4e07d3a85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8203e36b860f4ec1b2f29460fa2e769b",
      "placeholder": "​",
      "style": "IPY_MODEL_c142e8bc510d455dbb856e347c43002d",
      "value": " 2/2 [00:00&lt;00:00,  6.02it/s]"
     }
    },
    "8203e36b860f4ec1b2f29460fa2e769b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8413f90beaf34d1984dcb53d5f07da67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8efab274a37448799a998d884c82ee83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_39231b40984143b7b0dd9ed3e2c4961d",
      "placeholder": "​",
      "style": "IPY_MODEL_b3056923f49e40ff844b4cffd51d4f63",
      "value": " 180/1159 [00:35&lt;03:15,  5.00it/s, loss=5.81, v_num=0]"
     }
    },
    "922af2f87cc74257868c507bab3ceb49": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9383381c89e24eaa879bb2e7ceaaddaf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bc782d506310438b89d79d2fc77042c2",
       "IPY_MODEL_19a55abb050847298658a4283375005f",
       "IPY_MODEL_8efab274a37448799a998d884c82ee83"
      ],
      "layout": "IPY_MODEL_f6bd10c492bf4ce2ad0f4c5ae3efc85b"
     }
    },
    "98a691a38dd8477985e5234e360200f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c435bb03a6ab4ec8a40d4276732e6b0d",
      "placeholder": "​",
      "style": "IPY_MODEL_f3c144a9e9814525904ae136026171d6",
      "value": " 58/58 [00:05&lt;00:00, 10.45it/s]"
     }
    },
    "9b6e5626cf0d4cd3af6ecf9373221053": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a1449ea4e3e34c61b34667442e02a8e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a8a32c489f704efb9c1c2087646c114a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad4c364ea5774625a6b26ba6ffe41777": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a1449ea4e3e34c61b34667442e02a8e4",
      "max": 58,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c9f7a2c3f1b841259ef860f3746983aa",
      "value": 58
     }
    },
    "b3056923f49e40ff844b4cffd51d4f63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ba2264561be0481496ced94b317c1e50": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bc782d506310438b89d79d2fc77042c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0dc011835aca469d9de2b70f7dfc0547",
      "placeholder": "​",
      "style": "IPY_MODEL_9b6e5626cf0d4cd3af6ecf9373221053",
      "value": "Epoch 0:  16%"
     }
    },
    "c142e8bc510d455dbb856e347c43002d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c379c4e8fed247d2b238b1f62b50bbae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "c435bb03a6ab4ec8a40d4276732e6b0d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5df7f27e9614929bf3e3aef7280f5ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9752f70fb9849948585c5684d0de818": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c9f7a2c3f1b841259ef860f3746983aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "de9aa5adb43d4d389e9dbd7d77b36c01": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e53b960fd7c84783950d73bac29c8ba9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_752b1fd4196a4947ac30b277687e651a",
       "IPY_MODEL_ad4c364ea5774625a6b26ba6ffe41777",
       "IPY_MODEL_98a691a38dd8477985e5234e360200f9"
      ],
      "layout": "IPY_MODEL_7ac4f29341fe4dd59abaf7e5e9ff3c15"
     }
    },
    "f3c144a9e9814525904ae136026171d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f6bd10c492bf4ce2ad0f4c5ae3efc85b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "fae9f11a62144b6793279a736f04a259": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4c584691d1364f719010a37bd1571535",
       "IPY_MODEL_747cac49b92840b9814cb34b67e577c1",
       "IPY_MODEL_7b71954cf233487a800359c4e07d3a85"
      ],
      "layout": "IPY_MODEL_c379c4e8fed247d2b238b1f62b50bbae"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
