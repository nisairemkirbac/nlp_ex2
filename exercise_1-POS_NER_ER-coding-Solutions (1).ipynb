{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEtXEa5GlKyP"
   },
   "source": [
    "# POS with HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfSYG9bPlUld"
   },
   "source": [
    "In this notebook we will look into the task of Part Of Speech Tagging. We will see how to prepare data, implement an algorithm, and evaluate it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVUIGfpdlk7c"
   },
   "source": [
    "### Preparing Data\n",
    "\n",
    "Let us start by downloading a dataset and see what the data looks like. \n",
    "\n",
    "We make use of a sample from the *CONLL-2003++* dataset, from here: [https://github.com/ZihanWangKi/CrossWeigh\n",
    "](https://github.com/ZihanWangKi/CrossWeigh). I put the data in a data.zip, which we will open and process now. \n",
    "\n",
    "**LOCAL NOTEBOOK:**\n",
    "If you are opening this in a jupyter notebook, just make sure the zip is in the same directory. Skip the following cells till the next text block.\n",
    "\n",
    "**GOOGLE COLAB:**\n",
    "Are you working in a google colab, you need to give acces to google drive where you can put the zip with data. Run the following cells and follow the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33032,
     "status": "ok",
     "timestamp": 1635112465744,
     "user": {
      "displayName": "Victor Milewski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8ATR8MBxA3DRxgsWRfETNmC5Ng4t-z_MXil1pw=s64",
      "userId": "00980338011929053842"
     },
     "user_tz": -120
    },
    "id": "Pm2Kr9fIlnPh",
    "outputId": "951a389d-e244-41db-80b6-8a90bf531d01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2857,
     "status": "ok",
     "timestamp": 1635112468594,
     "user": {
      "displayName": "Victor Milewski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8ATR8MBxA3DRxgsWRfETNmC5Ng4t-z_MXil1pw=s64",
      "userId": "00980338011929053842"
     },
     "user_tz": -120
    },
    "id": "__ooYunsl917"
   },
   "outputs": [],
   "source": [
    "# MAKE SURE TO CHANGE THE PATH IN YOUR GOOGLE DRIVE TO THE DATAFILE. \n",
    "# !cp drive/MyDrive/[path to data in your personal google drive]/data.zip .\n",
    "!cp drive/MyDrive/KULEUVEN/teaching/NLP/'NLP 2021-2022'/Exercises/General_Track/exercise1_POS/data.zip .\n",
    "# !cp 'drive/MyDrive/1 PhD/Teaching/NLP/NLP 2021-2022/Exercises/General_Track/exercise1_POS/data.zip' ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_d7zo0Ftf2L"
   },
   "source": [
    "Let us unzip the data.zip file and have a look at what some of the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1635112468595,
     "user": {
      "displayName": "Victor Milewski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8ATR8MBxA3DRxgsWRfETNmC5Ng4t-z_MXil1pw=s64",
      "userId": "00980338011929053842"
     },
     "user_tz": -120
    },
    "id": "0wdKhIW_hHxc",
    "outputId": "d37d64ea-194e-43de-bde7-3262c49f0c34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data.zip\n",
      "   creating: data/\n",
      "  inflating: data/conllpp_dev.txt    \n",
      "  inflating: data/conllpp_test.txt   \n",
      "  inflating: data/conllpp_train.txt  \n"
     ]
    }
   ],
   "source": [
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 448,
     "status": "ok",
     "timestamp": 1635112469033,
     "user": {
      "displayName": "Victor Milewski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8ATR8MBxA3DRxgsWRfETNmC5Ng4t-z_MXil1pw=s64",
      "userId": "00980338011929053842"
     },
     "user_tz": -120
    },
    "id": "oGIumLSKhTov",
    "outputId": "bdb312c9-3367-49e8-9419-d4ddf7672200"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conllpp_dev.txt  conllpp_test.txt  conllpp_train.txt\n"
     ]
    }
   ],
   "source": [
    "ls data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzYS3cCYhcOa"
   },
   "source": [
    "Okay, so we have a training, a development, and a test split. Let's look inside one of the files.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1635112469035,
     "user": {
      "displayName": "Victor Milewski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8ATR8MBxA3DRxgsWRfETNmC5Ng4t-z_MXil1pw=s64",
      "userId": "00980338011929053842"
     },
     "user_tz": -120
    },
    "id": "X35i5DhomBYZ",
    "outputId": "65b5ebb2-7eb2-48bf-d33a-4d928ba9d57e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-DOCSTART-\t-X-\t-X-\tO\n",
      "\n",
      "CRICKET\tNNP\tB-NP\tO\n",
      "-\t:\tO\tO\n",
      "LEICESTERSHIRE\tNNP\tB-NP\tB-ORG\n",
      "TAKE\tNNP\tI-NP\tO\n",
      "OVER\tIN\tB-PP\tO\n",
      "AT\tNNP\tB-NP\tO\n",
      "TOP\tNNP\tI-NP\tO\n",
      "AFTER\tNNP\tI-NP\tO\n",
      "INNINGS\tNNP\tI-NP\tO\n",
      "VICTORY\tNN\tI-NP\tO\n",
      ".\t.\tO\tO\n",
      "\n",
      "LONDON\tNNP\tB-NP\tB-LOC\n",
      "1996-08-30\tCD\tI-NP\tO\n",
      "\n",
      "West\tNNP\tB-NP\tB-MISC\n",
      "Indian\tNNP\tI-NP\tI-MISC\n",
      "all-rounder\tNN\tI-NP\tO\n",
      "Phil\tNNP\tI-NP\tB-PER\n"
     ]
    }
   ],
   "source": [
    "with open('data/conllpp_dev.txt','r') as f:\n",
    "  for i,l in enumerate(f):\n",
    "    if i > 20:\n",
    "      break\n",
    "    print(\"\\t\".join(l.strip().split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlBed-mQuWHP"
   },
   "source": [
    "Each file contains sentences, where each line is a word and some annotations. The sentences are seperated by a white line. The first item on each line is a word, the second a part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GqnVsTqwsYQ"
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "Let's start by processing the data so it is a more useful format. We will put everything into lists, so that each item of the list is either the words of the sentence, or the labels of the sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 338,
     "status": "ok",
     "timestamp": 1635112469366,
     "user": {
      "displayName": "Victor Milewski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8ATR8MBxA3DRxgsWRfETNmC5Ng4t-z_MXil1pw=s64",
      "userId": "00980338011929053842"
     },
     "user_tz": -120
    },
    "id": "ueICmHXAtpWI"
   },
   "outputs": [],
   "source": [
    "def process_split(path_to_raw_txt):\n",
    "  # these lists will contain the full splits data\n",
    "  words = []\n",
    "  pos = []\n",
    "  chunks = []\n",
    "  ners = []\n",
    "  with open(path_to_raw_txt,'r') as f:\n",
    "    # we need buffers for collecting the current sentence\n",
    "    buffer_words = []\n",
    "    buffer_pos = [] \n",
    "    buffer_chunks = []\n",
    "    buffer_ners = []\n",
    "    # we will loop over all the lines in the file\n",
    "    for line in f:\n",
    "      line = line.strip()\n",
    "      #The file starts with a -DOCSTART- indicater. skip this line\n",
    "      if line.startswith('-DOCSTART-'):\n",
    "        continue\n",
    "      # if we reach a blank line, we add the complete buffer to the full splits lists\n",
    "      if len(line) == 0:\n",
    "        # make sure we don't add empty buffer to the data lists\n",
    "        if len(buffer_words) != 0:\n",
    "          words.append(buffer_words)\n",
    "          pos.append(buffer_pos)\n",
    "          chunks.append(buffer_chunks)\n",
    "          ners.append(buffer_ners)\n",
    "        # now we need to reset the buffers for the next sentence\n",
    "        buffer_words = []\n",
    "        buffer_pos = [] \n",
    "        buffer_chunks = []\n",
    "        buffer_ners = []\n",
    "      else:\n",
    "        # split the current line into the 4 elements and add them to the current sentence\n",
    "        elements = line.split()\n",
    "        buffer_words.append(elements[0].lower())\n",
    "        buffer_pos.append(elements[1])\n",
    "        buffer_chunks.append(elements[2])\n",
    "        buffer_ners.append(elements[3])\n",
    "    # if we finished the loop and we still have a sentence in our buffer, we add it to the full list\n",
    "    if len(buffer_words) != 0:\n",
    "      words.append(buffer_words)\n",
    "      pos.append(buffer_pos)\n",
    "      chunks.append(buffer_chunks)\n",
    "      ners.append(buffer_ners)\n",
    "  return words, pos, chunks, ners\n",
    "\n",
    "train_words, train_pos, train_chunks, train_ners = process_split('data/conllpp_train.txt')\n",
    "dev_words, dev_pos, dev_chunks, dev_ners = process_split('data/conllpp_dev.txt')\n",
    "test_words, test_pos, test_chunks, test_ners = process_split('data/conllpp_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1635112469367,
     "user": {
      "displayName": "Victor Milewski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8ATR8MBxA3DRxgsWRfETNmC5Ng4t-z_MXil1pw=s64",
      "userId": "00980338011929053842"
     },
     "user_tz": -120
    },
    "id": "qwDc3caDzSt3",
    "outputId": "819c643c-c525-4019-fd16-6050856a253b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS:  [['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.'], ['peter', 'blackburn'], ['brussels', '1996-08-22']]\n",
      "POS TAGS:  [['NNP', 'VBZ', 'JJ', 'NN', 'TO', 'VB', 'JJ', 'NN', '.'], ['NNP', 'NNP'], ['NNP', 'CD']]\n",
      "CHUNKS:  [['B-NP', 'B-VP', 'B-NP', 'I-NP', 'B-VP', 'I-VP', 'B-NP', 'I-NP', 'O'], ['B-NP', 'I-NP'], ['B-NP', 'I-NP']]\n",
      "NER TAGS:  [['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O'], ['B-PER', 'I-PER'], ['B-LOC', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(\"WORDS: \", train_words[:3])\n",
    "print(\"POS TAGS: \", train_pos[:3])\n",
    "print(\"CHUNKS: \", train_chunks[:3])\n",
    "print(\"NER TAGS: \", train_ners[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfAkDn-j1sX4"
   },
   "source": [
    "The data looks good now, and easily usable now. Now we start implementing a POS model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqaLjT4JA2LQ"
   },
   "source": [
    "## POS Tagging with a HMM\n",
    "\n",
    "For our HMM, we will need to prepare several probabilities for our markov chain. We will collect a set of possible labels and possible tokens from the train data. \n",
    "\n",
    "We add a start-of-sentence ('\\<SOS\\>') token at the beginning of every sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1635112939613,
     "user": {
      "displayName": "Victor Milewski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8ATR8MBxA3DRxgsWRfETNmC5Ng4t-z_MXil1pw=s64",
      "userId": "00980338011929053842"
     },
     "user_tz": -120
    },
    "id": "73i32d1yEXUD"
   },
   "outputs": [],
   "source": [
    "# import the needed modules\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 474,
     "status": "ok",
     "timestamp": 1635112470399,
     "user": {
      "displayName": "Victor Milewski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8ATR8MBxA3DRxgsWRfETNmC5Ng4t-z_MXil1pw=s64",
      "userId": "00980338011929053842"
     },
     "user_tz": -120
    },
    "id": "LZGR-zY7EiN5",
    "outputId": "f6395504-06f3-47a0-bcaa-b19509d38030"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Unique words: 21010\n",
      "Number of labels: 46, labels: Counter({'NNP': 34392, 'NN': 23899, 'CD': 19704, 'IN': 19064, '<SOS>': 14041, 'DT': 13453, 'JJ': 11831, 'NNS': 9903, 'VBD': 8293, '.': 7389, ',': 7291, 'VB': 4252, 'VBN': 4105, 'RB': 3975, 'CC': 3653, 'TO': 3469, 'PRP': 3163, '(': 2866, ')': 2866, 'VBG': 2585, 'VBZ': 2426, ':': 2386, '\"': 2178, 'POS': 1553, 'PRP$': 1520, 'VBP': 1436, 'MD': 1199, 'NNPS': 684, 'WP': 528, 'RP': 528, 'WDT': 506, 'SYM': 439, '$': 427, 'WRB': 384, 'JJR': 382, 'JJS': 254, 'FW': 166, 'RBR': 163, 'EX': 136, 'RBS': 35, \"''\": 35, 'PDT': 33, 'UH': 30, 'WP$': 23, 'LS': 13, 'NN|SYM': 4})\n",
      "Number of transitions: 1143, bigrams: Counter({('NNP', 'NNP'): 11130, ('CD', 'CD'): 7259, ('NN', 'IN'): 6426, ('IN', 'DT'): 5999, ('DT', 'NN'): 5855, ('JJ', 'NN'): 5428, ('<SOS>', 'NNP'): 5421, ('IN', 'NNP'): 4518, ('NNP', 'CD'): 4002, ('DT', 'JJ'): 3634, ('NN', 'NN'): 2882, ('NNS', 'IN'): 2708, ('NNP', ','): 2343, ('NN', '.'): 2241, ('TO', 'VB'): 1982, ('NNP', 'VBD'): 1949, ('(', 'NNP'): 1904, ('JJ', 'NNS'): 1880, ('NN', ','): 1847, ('NNP', '('): 1838, ('IN', 'NN'): 1788, ('NNP', ')'): 1788, ('NNP', 'IN'): 1758, ('CD', 'NNS'): 1718, ('NNP', '.'): 1695, ('DT', 'NNP'): 1642, ('NNP', 'NN'): 1590, (',', 'NNP'): 1560, ('NN', 'VBD'): 1525, ('IN', 'JJ'): 1471, ('VBN', 'IN'): 1448, ('CD', 'NNP'): 1397, ('IN', 'CD'): 1384, ('PRP', 'VBD'): 1381, ('VBD', 'DT'): 1351, ('<SOS>', 'DT'): 1300, ('NN', 'NNS'): 1234, ('VBD', 'IN'): 1212, ('<SOS>', 'NN'): 1098, ('NNS', '.'): 1079, ('JJ', 'JJ'): 1072, ('<SOS>', 'CD'): 1071, (':', 'NNP'): 1061, ('CD', 'NN'): 1024, ('NNS', 'VBD'): 1004, ('MD', 'VB'): 970, ('VBD', 'VBN'): 970, ('NNP', 'POS'): 947, ('NN', 'NNP'): 946, ('NN', 'CC'): 946, ('NNS', ','): 875, (')', 'CD'): 851, ('DT', 'NNS'): 839, ('<SOS>', 'NNS'): 819, ('NN', 'TO'): 789, ('VB', 'DT'): 789, ('<SOS>', 'JJ'): 775, ('JJ', 'NNP'): 768, ('IN', 'NNS'): 767, ('NNP', 'CC'): 738, ('PRP$', 'NN'): 699, ('IN', 'PRP$'): 689, (',', 'DT'): 678, ('NN', 'VBZ'): 670, ('VBD', '.'): 668, ('NN', 'CD'): 667, ('RB', 'IN'): 662, ('CD', ','): 658, ('CC', 'NNP'): 655, ('<SOS>', '\"'): 640, (',', 'VBD'): 634, ('IN', 'VBG'): 631, ('VBD', 'RB'): 628, (',', '\"'): 626, ('CD', 'IN'): 604, ('NNS', 'CC'): 600, ('NNP', 'VBZ'): 596, ('JJ', 'IN'): 594, ('<SOS>', 'PRP'): 578, ('NNP', 'NNS'): 574, ('POS', 'NN'): 556, ('NNP', ':'): 556, ('VBD', 'NNP'): 555, ('<SOS>', 'IN'): 535, ('CD', '.'): 528, ('CD', ')'): 509, ('VBZ', 'VBN'): 508, ('VB', 'IN'): 503, ('IN', 'PRP'): 479, ('VB', 'NNP'): 479, ('PRP', 'VBP'): 463, ('PRP$', 'JJ'): 460, ('(', 'CD'): 457, ('VBG', 'DT'): 440, ('CD', 'JJ'): 435, ('TO', 'DT'): 434, ('NNS', 'VBP'): 428, ('NN', 'POS'): 422, ('\"', 'PRP'): 419, ('$', 'CD'): 416, ('CD', ':'): 413, ('RB', 'VB'): 409, ('DT', 'CD'): 409, ('<SOS>', ':'): 405, ('CC', 'DT'): 404, ('.', '\"'): 403, ('VBD', 'JJ'): 396, ('VBN', 'TO'): 390, ('NNP', 'JJ'): 389, ('NN', ':'): 386, ('VBG', 'IN'): 385, (',', 'VBG'): 384, ('VBD', 'CD'): 383, ('RB', 'VBN'): 376, ('CC', 'VBD'): 371, ('CC', 'JJ'): 363, ('VB', 'VBN'): 354, ('POS', 'JJ'): 352, (',', 'CC'): 351, ('VBD', 'TO'): 349, (',', 'IN'): 348, ('PRP', 'MD'): 339, ('RB', 'JJ'): 339, ('JJ', 'CD'): 336, ('CC', 'NN'): 333, ('NNS', 'TO'): 331, ('VBD', 'PRP'): 330, ('CD', '('): 330, ('IN', 'IN'): 326, ('VBZ', 'RB'): 317, ('VBN', 'DT'): 315, (',', 'CD'): 313, ('TO', 'NNP'): 311, ('NN', 'RB'): 310, ('PRP', 'VBZ'): 308, ('NNP', 'NNPS'): 305, ('JJ', ','): 297, ('VBP', 'VBN'): 296, ('\"', 'NNP'): 295, ('VBZ', 'DT'): 295, ('RB', 'VBD'): 292, ('VBG', 'NN'): 286, (')', 'NNP'): 285, ('RB', 'CD'): 279, (',', 'NN'): 278, (')', 'VB'): 278, ('NN', 'JJ'): 277, ('VB', 'NN'): 275, ('WP', 'VBD'): 267, ('JJ', 'TO'): 266, ('VBD', 'NN'): 262, ('POS', 'NNP'): 261, (',', 'WP'): 256, ('\"', 'DT'): 255, ('VBN', 'VBN'): 255, (',', 'RB'): 253, ('VB', 'JJ'): 250, ('NNP', 'TO'): 248, ('NN', 'VBN'): 248, ('NN', 'MD'): 244, ('TO', 'CD'): 244, (')', ','): 241, (',', 'JJ'): 238, ('NNS', ':'): 233, (',', 'NNS'): 232, ('NNS', 'VBN'): 229, ('CC', 'NNS'): 227, ('VBD', 'VBG'): 226, ('CC', 'CD'): 225, ('RB', ','): 224, ('VBP', 'RB'): 223, ('RB', 'RB'): 221, ('VBZ', 'IN'): 219, ('VB', 'CD'): 211, ('PRP$', 'NNS'): 209, ('NNS', 'NN'): 207, (':', 'CD'): 206, ('CC', 'PRP'): 206, (',', 'PRP'): 202, ('JJ', 'CC'): 201, ('VBD', 'NNS'): 200, ('NNS', 'RB'): 200, ('NN', '('): 199, ('JJ', '.'): 198, ('NN', 'VBG'): 197, ('VBN', 'NN'): 197, ('MD', 'RB'): 197, ('<SOS>', 'RB'): 195, ('NN', ')'): 195, ('SYM', 'NNP'): 188, ('VBG', 'TO'): 187, ('NNP', 'SYM'): 185, ('RB', '.'): 185, (',', 'VBN'): 185, ('<SOS>', 'VBG'): 183, ('POS', 'NNS'): 182, ('VBZ', 'JJ'): 181, ('VBD', 'PRP$'): 178, ('NNS', 'CD'): 177, ('VB', 'TO'): 176, ('NN', 'DT'): 175, ('VBG', 'NNP'): 172, ('CD', 'CC'): 172, ('VBG', 'JJ'): 171, ('VBG', 'CD'): 171, ('VBN', '.'): 169, ('NNS', 'MD'): 164, ('VBN', 'JJ'): 164, (',', 'VBZ'): 164, ('<SOS>', 'CC'): 163, ('NNP', 'RB'): 163, ('VB', 'PRP$'): 162, ('VBN', ','): 162, ('NNS', '('): 161, (',', 'WDT'): 161, ('VB', 'NNS'): 160, (')', 'IN'): 159, ('IN', 'RB'): 158, ('NNP', 'MD'): 158, ('VBD', ','): 158, ('RB', 'DT'): 157, ('VBG', 'NNS'): 157, ('WDT', 'VBD'): 156, ('NNS', 'POS'): 155, (')', '.'): 155, ('NN', 'WDT'): 154, ('RP', 'IN'): 151, ('\"', 'VBD'): 150, ('VB', 'RB'): 149, ('IN', '$'): 149, ('VBP', 'DT'): 148, ('VBD', 'RP'): 148, ('VBN', 'RB'): 147, ('<SOS>', 'VB'): 146, ('<SOS>', '('): 145, ('CC', 'RB'): 144, ('(', 'NN'): 143, ('VBN', 'NNP'): 142, ('\"', 'IN'): 142, ('TO', 'NN'): 140, ('VB', 'PRP'): 137, ('VBN', 'CD'): 136, ('PRP', 'IN'): 134, ('NNS', 'NNP'): 134, ('JJR', 'IN'): 134, ('WRB', 'PRP'): 133, ('VBZ', 'NN'): 131, ('NNS', 'JJ'): 130, ('WDT', 'VBZ'): 130, ('NNS', ')'): 129, ('VBZ', 'NNP'): 128, ('CC', 'VB'): 127, ('PRP', 'RB'): 125, ('VBP', 'IN'): 124, ('NN', 'PRP'): 123, ('RB', 'TO'): 122, ('NNS', 'VBG'): 122, ('RB', 'VBG'): 120, (')', ':'): 119, ('VBN', 'NNS'): 118, ('DT', 'VBG'): 117, ('CD', 'RB'): 117, ('DT', 'VBN'): 112, ('NNP', 'DT'): 111, ('VBZ', 'VBG'): 111, ('VBP', 'JJ'): 109, ('NNP', 'VBP'): 109, ('CC', 'IN'): 108, ('VBZ', 'TO'): 106, ('JJ', ')'): 106, ('VB', 'RP'): 106, ('DT', 'NNPS'): 105, ('VB', '.'): 105, ('(', 'JJ'): 105, (':', 'JJ'): 103, ('CD', 'TO'): 99, ('TO', 'JJ'): 98, ('WRB', 'DT'): 98, ('RP', 'DT'): 96, ('DT', 'IN'): 95, ('NN', '\"'): 95, ('NNP', 'PRP'): 93, ('NNPS', 'NNP'): 93, ('VBP', 'VBG'): 92, (':', 'NN'): 92, ('VBN', 'RP'): 90, ('VBG', 'RB'): 89, ('\"', 'NN'): 89, ('<SOS>', 'VBN'): 88, ('VBG', 'VBN'): 86, ('VBZ', 'CD'): 86, ('POS', 'CD'): 86, ('NN', 'WRB'): 86, ('JJR', 'NN'): 86, ('CC', 'VBZ'): 85, ('VBG', 'PRP$'): 85, ('NNS', 'NNS'): 84, ('DT', 'JJS'): 84, ('FW', 'NNP'): 83, ('RB', 'NN'): 83, ('DT', 'RB'): 82, ('JJS', 'NN'): 81, ('VBP', 'TO'): 79, ('WP', 'VBZ'): 79, ('IN', 'VBN'): 78, ('CC', 'VBG'): 78, ('NNS', 'WDT'): 76, ('IN', ','): 75, ('VBN', 'CC'): 74, ('RB', 'VBZ'): 71, ('CC', 'VBN'): 71, ('NNPS', 'IN'): 71, ('NNS', 'WP'): 71, ('VBG', 'PRP'): 70, ('NNP', 'FW'): 69, ('SYM', 'NN'): 69, (')', 'VBD'): 68, ('\"', 'JJ'): 67, ('PRP', '.'): 67, ('JJ', 'VBD'): 66, ('RB', 'CC'): 66, ('NNP', 'VBG'): 65, ('NN', 'VBP'): 65, ('NN', 'WP'): 65, (',', 'WRB'): 64, ('VB', ','): 64, ('NNPS', ','): 64, (',', 'MD'): 64, ('NNPS', 'VBD'): 63, ('VBN', 'VBG'): 63, ('NNS', 'DT'): 62, ('DT', 'VBZ'): 61, ('CC', 'PRP$'): 60, ('NNPS', 'CD'): 60, ('CD', 'VBD'): 59, ('<SOS>', 'SYM'): 59, (')', 'NN'): 58, ('IN', 'NNPS'): 58, ('IN', 'WDT'): 58, ('VB', 'VBG'): 57, ('JJ', 'DT'): 57, ('WP', 'VBP'): 57, ('DT', 'JJR'): 57, ('PRP', 'TO'): 57, ('VBN', 'PRP'): 56, ('EX', 'VBD'): 56, (':', '\"'): 56, ('VBG', 'RP'): 56, ('PRP$', 'NNP'): 56, ('(', 'IN'): 56, ('<SOS>', 'TO'): 56, ('PRP', ','): 55, ('IN', 'TO'): 55, ('CD', 'SYM'): 55, (':', 'DT'): 54, ('TO', 'NNS'): 53, ('VBN', 'PRP$'): 53, (':', 'NNS'): 53, ('RB', 'NNP'): 52, ('NNS', 'SYM'): 52, ('WDT', 'MD'): 52, (':', 'VB'): 52, (')', 'JJ'): 52, ('IN', 'JJR'): 52, ('JJ', 'RB'): 52, ('JJS', 'IN'): 52, ('DT', 'VBD'): 51, ('VBP', 'NN'): 51, ('IN', 'JJS'): 51, ('TO', 'PRP$'): 50, ('VB', 'CC'): 50, ('NNP', 'VBN'): 49, ('JJR', 'NNS'): 49, ('<SOS>', 'VBD'): 49, ('JJ', '('): 49, ('CD', 'NNPS'): 49, ('NNS', 'PRP'): 48, ('EX', 'VBZ'): 47, ('NNP', '\"'): 47, ('NNPS', '.'): 46, ('RB', 'VBP'): 46, ('NNP', 'WRB'): 46, ('VBZ', 'PRP'): 46, (':', ':'): 46, ('IN', '\"'): 45, ('VBG', '.'): 45, ('VBP', 'PRP'): 45, ('DT', '\"'): 44, ('<SOS>', 'PRP$'): 44, ('SYM', 'CD'): 43, ('WP', 'PRP'): 42, ('NNS', 'VBZ'): 42, ('VBZ', 'NNS'): 42, ('WRB', 'NNP'): 42, ('JJ', ':'): 42, ('CD', 'VBZ'): 42, ('<SOS>', 'JJR'): 41, ('\"', 'RB'): 41, ('JJ', 'VBN'): 41, ('NNPS', 'CC'): 41, ('\"', '.'): 41, ('PRP$', 'CD'): 41, ('RB', 'PRP'): 41, ('RBR', 'IN'): 40, ('VBP', 'NNS'): 40, ('(', '$'): 39, ('NN', 'RBR'): 39, ('NNPS', 'VBP'): 39, ('VB', ':'): 39, ('PRP', 'VB'): 38, ('RB', 'NNS'): 38, ('RB', 'RP'): 38, ('NNS', '\"'): 38, (',', 'VBP'): 37, ('CC', 'MD'): 37, ('VBD', ':'): 37, ('NN', 'VB'): 37, ('JJ', '\"'): 37, ('VBD', 'JJR'): 36, ('CD', 'DT'): 36, (')', 'RB'): 36, ('JJS', 'CD'): 36, ('NN', 'SYM'): 35, ('RP', ','): 35, ('VBZ', '.'): 35, ('RBR', 'DT'): 35, (')', 'CC'): 35, ('TO', '$'): 35, ('VBG', ':'): 34, ('(', 'VBN'): 34, ('VBZ', ')'): 34, ('WRB', 'JJ'): 33, ('<SOS>', 'EX'): 33, ('PRP', 'CC'): 33, ('WDT', 'VBP'): 33, ('JJS', 'NNS'): 33, ('VBP', 'NNP'): 32, ('<SOS>', 'FW'): 32, ('RB', 'JJR'): 32, ('RP', 'CD'): 32, ('SYM', 'NNS'): 32, ('DT', 'VBP'): 31, ('\"', 'NNS'): 31, ('PDT', 'DT'): 31, ('(', 'NNS'): 31, ('IN', 'WP'): 31, ('VBD', 'CC'): 31, ('NNS', 'VB'): 30, ('NNPS', '('): 30, ('RBR', 'JJ'): 30, ('VBZ', 'PRP$'): 29, ('\"', 'CD'): 29, ('<SOS>', 'NNPS'): 29, ('\"', 'CC'): 29, ('POS', 'JJS'): 29, ('VBD', 'RBR'): 28, ('PRP', 'DT'): 28, (',', 'TO'): 28, ('JJ', 'NNPS'): 27, ('VBG', ','): 27, ('VB', 'WRB'): 27, ('WDT', 'PRP'): 27, ('NNP', 'WDT'): 26, ('RBS', 'JJ'): 26, ('RP', 'NN'): 26, ('NNPS', 'NN'): 26, ('RP', '.'): 26, ('VBP', 'CD'): 26, ('CD', 'VBG'): 26, ('VBG', 'CC'): 26, ('RP', 'TO'): 26, ('TO', 'PRP'): 25, ('VBP', 'VBD'): 25, ('\"', 'EX'): 25, (':', 'IN'): 25, ('DT', '$'): 25, ('JJ', 'VBZ'): 25, ('PRP', 'JJ'): 25, ('CD', 'VBN'): 24, ('NNP', 'VB'): 24, ('RP', 'PRP$'): 24, ('CD', 'VBP'): 24, ('<SOS>', 'JJS'): 24, (')', 'NNS'): 24, (',', 'PRP$'): 23, ('CC', 'VBP'): 23, (',', 'VB'): 23, ('JJ', 'VBG'): 23, ('VBD', '\"'): 23, ('\"', ','): 22, ('NN', 'PRP$'): 22, ('VB', '\"'): 22, ('TO', 'RB'): 22, ('VBP', ','): 22, ('NNPS', ')'): 22, ('WDT', 'NNP'): 21, ('VB', 'JJR'): 21, ('DT', 'RBS'): 21, ('<SOS>', 'WRB'): 21, ('VBD', '$'): 21, ('VBP', 'RP'): 21, ('(', 'NNPS'): 21, ('RP', 'NNP'): 21, ('RP', 'JJ'): 21, ('VBP', 'PRP$'): 21, ('IN', 'EX'): 20, ('EX', 'VBP'): 20, ('IN', 'CC'): 20, ('\"', 'PRP$'): 19, ('(', 'DT'): 19, ('CD', 'JJR'): 19, ('NN', '$'): 19, ('NNS', 'WRB'): 19, ('DT', 'MD'): 19, ('PRP', 'VBN'): 19, ('VBN', 'VBD'): 19, ('DT', 'DT'): 19, ('CC', 'TO'): 19, ('CC', 'EX'): 19, ('CD', 'WRB'): 19, ('VB', 'VB'): 18, ('RP', 'NNS'): 18, ('JJR', ','): 18, ('DT', '.'): 18, ('WP', 'MD'): 18, ('VBN', 'WRB'): 18, ('WDT', 'DT'): 17, ('IN', 'WRB'): 17, ('NNPS', 'TO'): 17, (')', 'DT'): 17, (':', 'CC'): 17, ('TO', 'VBG'): 17, (')', 'SYM'): 17, ('VBP', '.'): 17, (':', 'PRP'): 17, ('VBZ', ','): 17, ('VBD', 'VBD'): 17, ('NNP', 'WP'): 17, ('\"', 'VBG'): 17, ('JJR', 'CD'): 17, ('RB', 'MD'): 17, ('VBG', '\"'): 16, ('<SOS>', 'UH'): 16, ('NNS', 'RBR'): 16, ('WDT', 'RB'): 16, ('JJ', 'SYM'): 16, (':', '$'): 16, ('JJ', 'PRP'): 16, ('WP', 'RB'): 16, ('\"', 'VB'): 16, ('VBD', 'EX'): 16, ('JJR', 'JJ'): 15, ('VBZ', 'VBD'): 15, ('NNP', '$'): 15, ('DT', 'VB'): 15, ('RP', ')'): 15, ('PRP$', 'JJS'): 15, ('IN', '.'): 15, ('WRB', 'NNS'): 15, ('RB', '$'): 14, ('NNPS', 'POS'): 14, ('CD', 'VB'): 14, ('CC', 'JJR'): 14, ('NNP', 'PRP$'): 14, ('FW', 'CD'): 14, ('DT', ','): 14, ('JJ', 'WRB'): 14, ('VBZ', 'CC'): 14, ('VBN', ')'): 14, ('PRP', 'NN'): 14, ('RB', ':'): 14, ('RBR', ','): 14, ('JJS', 'JJ'): 14, (',', 'WP$'): 14, ('FW', 'NN'): 14, ('DT', '('): 14, ('\"', 'VBZ'): 13, ('DT', 'WP'): 13, ('CD', '$'): 13, ('RB', 'WRB'): 13, ('CC', '$'): 13, (')', 'VBZ'): 13, (')', '('): 13, ('RB', 'RBR'): 12, ('\"', 'VBN'): 12, ('WDT', 'NNS'): 12, ('RP', 'RB'): 12, ('JJ', 'JJS'): 12, (')', 'TO'): 12, ('IN', 'VBD'): 12, ('SYM', 'DT'): 12, ('WRB', 'NN'): 12, ('NN', 'JJR'): 12, ('<SOS>', ','): 12, ('TO', 'IN'): 12, ('VBG', 'JJR'): 12, ('(', 'JJR'): 12, ('NN', 'RP'): 12, ('VBZ', 'JJR'): 12, ('PRP', 'RP'): 12, ('VB', 'VBD'): 12, ('IN', 'PDT'): 12, ('RBR', '.'): 11, ('JJ', 'VBP'): 11, ('VB', 'NNPS'): 11, ('PRP', 'CD'): 11, ('JJ', 'RP'): 11, ('JJ', 'MD'): 11, ('WP', 'VB'): 11, (',', '$'): 11, ('VBN', '\"'): 11, (')', 'VBP'): 11, ('CD', 'MD'): 11, ('JJR', '.'): 10, ('IN', 'VBZ'): 10, ('VBZ', 'WRB'): 10, ('VBZ', 'VBZ'): 10, ('\"', 'TO'): 10, (':', 'VBN'): 10, ('NN', 'FW'): 10, ('POS', 'VBN'): 10, ('VBG', 'VBD'): 10, ('WP$', 'NN'): 10, ('VBD', 'WRB'): 10, ('RB', '\"'): 10, ('JJ', '$'): 10, ('VBP', 'VBZ'): 10, ('VB', 'WP'): 10, ('TO', '\"'): 10, ('POS', 'RB'): 10, ('NNS', 'JJR'): 10, ('SYM', 'RB'): 10, ('DT', 'RBR'): 10, ('\"', 'WP'): 9, ('VBZ', '$'): 9, ('NN', 'NNPS'): 9, ('PRP$', 'VBG'): 9, ('IN', 'VB'): 9, ('VBG', 'VBP'): 9, ('VBD', 'VB'): 9, ('IN', ')'): 9, ('VBN', ':'): 9, ('EX', 'MD'): 9, ('SYM', 'JJ'): 9, ('JJ', 'PRP$'): 9, ('RB', 'PRP$'): 9, ('IN', 'RBR'): 9, ('<SOS>', 'RBR'): 9, ('VBN', 'VBZ'): 9, ('CC', ','): 9, ('VBP', ':'): 8, ('VBZ', 'RP'): 8, ('UH', 'NNP'): 8, ('RBR', 'TO'): 8, ('NNPS', 'NNPS'): 8, ('NNPS', ':'): 8, ('NNP', 'RBR'): 8, ('<SOS>', '$'): 8, ('VB', '('): 8, (',', 'JJS'): 8, ('VBP', 'JJR'): 8, (',', 'NNPS'): 8, ('\"', '('): 8, ('JJR', 'CC'): 8, ('UH', ','): 8, ('\"', 'MD'): 8, (',', \"''\"): 8, ('CC', '\"'): 8, ('VBN', 'RBR'): 8, ('JJ', 'JJR'): 8, ('.', \"''\"): 8, ('CC', 'NNPS'): 8, ('FW', 'FW'): 8, ('VBP', 'CC'): 8, ('DT', 'CC'): 8, ('VBG', 'VB'): 8, ('VBZ', ':'): 8, ('FW', 'NNS'): 8, ('RBR', 'VBN'): 7, ('NNPS', 'VBG'): 7, ('VBZ', '\"'): 7, (':', 'NNPS'): 7, (':', '('): 7, ('WRB', 'PRP$'): 7, ('PRP', '('): 7, ('LS', ')'): 7, ('(', 'TO'): 7, ('VB', ')'): 7, ('VBG', ')'): 7, ('JJ', 'VB'): 7, ('POS', 'VBZ'): 7, ('VBZ', 'WP'): 7, ('VBD', '('): 7, ('DT', 'PRP$'): 7, ('CD', 'PRP'): 7, ('PRP$', 'RB'): 7, ('POS', '.'): 7, ('VB', 'RBR'): 7, ('WDT', 'JJ'): 7, ('NN', 'EX'): 7, (',', 'EX'): 7, ('WRB', 'CD'): 7, ('VBG', 'VBG'): 7, (':', 'VBD'): 7, ('PRP', 'NNS'): 7, ('JJR', 'VBD'): 7, ('NNPS', 'VBN'): 7, ('<SOS>', 'VBZ'): 7, ('VB', '$'): 7, ('JJR', 'NNP'): 7, (':', 'RB'): 7, ('WP', 'DT'): 7, ('WP$', 'NNS'): 7, ('WDT', 'CD'): 7, ('FW', 'VBD'): 7, ('WDT', 'IN'): 6, ('<SOS>', 'WDT'): 6, ('SYM', 'VBN'): 6, ('MD', 'NNP'): 6, (':', 'VBG'): 6, ('NNPS', 'MD'): 6, ('NNS', 'RP'): 6, (')', 'MD'): 6, ('NNPS', 'JJ'): 6, ('UH', '.'): 6, (':', 'LS'): 6, ('LS', '.'): 6, ('FW', '('): 6, ('JJS', 'NNP'): 6, ('VBZ', 'VB'): 6, ('JJR', ':'): 6, ('IN', ':'): 6, ('PRP', '\"'): 6, ('POS', '\"'): 6, (',', 'UH'): 6, ('CD', 'WDT'): 6, ('JJR', 'RB'): 6, ('POS', 'VBG'): 6, ('RP', 'VBG'): 6, ('VB', 'MD'): 6, (',', 'FW'): 6, (',', '('): 6, ('JJ', 'FW'): 6, ('VBD', 'MD'): 6, ('CC', 'WP'): 6, ('WRB', 'VBN'): 6, ('\"', 'WRB'): 6, ('\"', ':'): 6, ('CC', 'WRB'): 6, ('TO', 'JJR'): 6, ('VBN', 'JJR'): 6, ('WDT', 'NN'): 6, ('VBD', 'JJS'): 6, ('<SOS>', '.'): 5, ('VBP', 'MD'): 5, ('<SOS>', 'VBP'): 5, ('DT', 'TO'): 5, ('(', 'VB'): 5, (')', 'FW'): 5, ('<SOS>', 'RP'): 5, ('NNPS', 'DT'): 5, ('VBN', '('): 5, ('JJS', 'VBD'): 5, ('JJ', 'WP'): 5, ('CC', 'JJS'): 5, ('CC', 'WDT'): 5, ('WRB', 'TO'): 5, (\"''\", ','): 5, ('TO', 'VBP'): 5, ('VBN', 'VB'): 5, ('JJ', 'POS'): 5, ('MD', ','): 5, ('WRB', 'IN'): 5, (',', 'JJR'): 5, ('WDT', '\"'): 5, ('VBN', '$'): 5, ('RBR', 'RB'): 5, ('DT', 'PRP'): 5, ('$', 'JJ'): 5, ('RP', 'CC'): 5, ('POS', '('): 5, ('TO', 'WP'): 5, ('VBZ', '('): 5, ('<SOS>', 'WP'): 5, ('VBG', 'NNPS'): 5, ('VBD', 'NNPS'): 5, ('NNS', 'WP$'): 5, ('FW', ','): 5, ('(', 'FW'): 5, ('JJR', 'VBN'): 4, ('VBZ', 'NNPS'): 4, ('WP', 'IN'): 4, ('VBG', 'WP'): 4, ('WP', 'NN'): 4, ('WRB', 'VBD'): 4, (':', 'WDT'): 4, ('NN', \"''\"): 4, (':', 'VBZ'): 4, ('JJS', 'TO'): 4, ('FW', 'JJ'): 4, ('CC', 'SYM'): 4, ('SYM', 'CC'): 4, ('RB', '('): 4, ('POS', 'VBD'): 4, ('MD', 'PRP'): 4, ('VBZ', 'RBR'): 4, ('CD', 'WP'): 4, ('VBG', '('): 4, ('POS', '$'): 4, ('NNS', 'PRP$'): 4, (':', 'TO'): 4, ('TO', ','): 4, ('VB', 'PDT'): 4, ('(', 'LS'): 4, ('$', 'IN'): 4, ('NNP', 'RP'): 4, ('NNS', 'FW'): 4, ('CD', 'FW'): 4, ('PRP$', '\"'): 4, ('VBG', '$'): 4, (':', 'MD'): 4, ('RB', 'PDT'): 4, ('(', 'RB'): 4, ('RB', ')'): 4, ('VBD', ')'): 4, (\"''\", 'POS'): 4, ('<SOS>', \"''\"): 4, ('POS', ':'): 4, ('SYM', 'NNPS'): 4, ('POS', ')'): 4, ('WP', 'NNS'): 4, (')', 'NNPS'): 4, ('JJS', 'RB'): 4, ('WDT', 'VBN'): 4, ('NNPS', 'WP'): 4, ('NN|SYM', 'CD'): 4, ('NNP', 'EX'): 3, ('NNS', '$'): 3, ('\"', 'VBP'): 3, ('JJ', \"''\"): 3, (\"''\", 'CD'): 3, (\"''\", 'NNP'): 3, (':', 'FW'): 3, (')', 'WDT'): 3, ('VBP', 'WP'): 3, ('FW', 'IN'): 3, ('DT', ')'): 3, ('JJS', ','): 3, ('NNPS', 'VBZ'): 3, ('TO', 'NNPS'): 3, ('VBP', 'SYM'): 3, ('PRP', 'WRB'): 3, ('NNPS', 'VB'): 3, ('WRB', 'MD'): 3, ('WDT', ','): 3, ('VBZ', 'RBS'): 3, ('PRP', 'VBG'): 3, ('VBN', 'WP'): 3, ('WP', 'NNP'): 3, ('PRP', 'PRP'): 3, ('WRB', 'JJR'): 3, ('PRP$', 'TO'): 3, ('JJR', 'VBG'): 3, ('JJS', 'VBG'): 3, ('(', 'VBG'): 3, ('MD', '\"'): 3, ('CD', 'POS'): 3, (',', 'RBR'): 3, ('(', 'VBZ'): 3, ('DT', 'RP'): 3, ('PRP', 'PRP$'): 3, (')', 'VBN'): 3, ('NNPS', 'FW'): 3, ('VB', 'WDT'): 3, ('WP', 'TO'): 3, ('RP', '$'): 3, ('NNPS', '\"'): 3, ('VBP', 'NNPS'): 3, ('MD', 'TO'): 3, ('VBZ', 'MD'): 3, ('RBR', 'NN'): 3, ('PRP$', 'IN'): 3, ('VBG', 'VBZ'): 3, ('WRB', 'RB'): 3, ('NNPS', 'RB'): 3, ('RB', 'WP'): 3, ('FW', '.'): 3, ('PRP', 'JJR'): 3, ('POS', 'IN'): 3, ('CC', 'CC'): 3, ('WP', 'VBN'): 3, ('NN', 'JJS'): 3, ('PRP$', 'JJR'): 3, ('POS', 'RBS'): 3, ('RBS', 'VBD'): 3, ('TO', 'VBZ'): 3, ('VBP', 'EX'): 3, ('NNP', 'JJR'): 3, ('PRP', 'NNP'): 3, ('VB', 'JJS'): 3, ('WP$', 'JJ'): 3, ('JJR', 'VB'): 3, ('CD', '\"'): 3, (':', 'WP'): 3, ('PRP', 'SYM'): 3, (')', 'NN|SYM'): 3, ('TO', '('): 2, ('IN', '('): 2, ('IN', 'MD'): 2, ('IN', 'UH'): 2, ('NNP', 'PDT'): 2, ('MD', 'IN'): 2, ('IN', 'VBP'): 2, ('\"', 'FW'): 2, ('FW', 'VBN'): 2, ('<SOS>', 'LS'): 2, ('PRP$', 'NNPS'): 2, ('NNPS', 'RP'): 2, ('JJS', 'VBZ'): 2, ('JJS', 'NNPS'): 2, ('VBP', 'FW'): 2, ('RBS', 'RB'): 2, ('<SOS>', 'PDT'): 2, ('SYM', 'IN'): 2, ('DT', ':'): 2, ('VBN', 'JJS'): 2, ('NN', 'WP$'): 2, ('SYM', 'VBP'): 2, ('VBG', 'MD'): 2, ('VBP', 'VB'): 2, ('CD', 'RP'): 2, (\"''\", '\"'): 2, ('(', 'VBP'): 2, ('VBP', ')'): 2, ('JJS', 'VB'): 2, ('NNP', \"''\"): 2, ('RP', '\"'): 2, ('VBG', 'WRB'): 2, ('PRP$', 'VBN'): 2, ('WRB', 'VBG'): 2, ('RP', 'VBN'): 2, ('SYM', 'VBZ'): 2, ('VBD', 'SYM'): 2, ('POS', ','): 2, ('$', 'NNP'): 2, (':', 'PRP$'): 2, ('PRP', 'RBR'): 2, ('TO', 'VBN'): 2, ('PRP', ':'): 2, ('RBR', 'VBD'): 2, (':', 'VBP'): 2, ('CC', 'FW'): 2, ('VBP', '\"'): 2, ('MD', 'NN'): 2, (')', '$'): 2, ('UH', 'CD'): 2, ('<SOS>', ')'): 2, (')', 'RP'): 2, ('MD', 'MD'): 2, ('POS', 'NNPS'): 2, ('PRP$', 'DT'): 2, ('\"', 'UH'): 2, ('FW', 'CC'): 2, ('POS', 'PRP'): 2, ('WP', 'CD'): 2, ('CC', 'PDT'): 2, ('JJS', '.'): 2, ('POS', 'CC'): 2, ('RBR', 'VB'): 2, ('JJR', 'DT'): 2, ('NNPS', 'NNS'): 2, ('WP', ','): 2, ('CC', '('): 2, ('WRB', '.'): 2, ('WDT', 'NNPS'): 2, ('WRB', 'JJS'): 2, ('\"', 'WDT'): 2, ('VBD', 'RBS'): 2, ('NNPS', 'PRP'): 2, ('MD', '.'): 2, ('PDT', 'PRP$'): 2, ('VB', 'VBP'): 2, (\"''\", 'NNS'): 2, ('JJ', 'RBR'): 2, ('(', 'PRP'): 2, ('TO', 'JJS'): 2, ('VBZ', 'VBP'): 2, ('JJR', 'TO'): 2, ('<SOS>', 'MD'): 2, ('PRP$', 'RBS'): 2, ('(', 'JJS'): 2, ('VB', 'VBZ'): 2, ('VBG', 'PDT'): 2, ('SYM', 'JJR'): 2, ('UH', 'SYM'): 2, ('MD', 'SYM'): 1, (')', '\"'): 1, ('JJS', 'PRP'): 1, ('FW', 'JJR'): 1, ('JJR', 'PRP$'): 1, ('<SOS>', 'RBS'): 1, (':', 'SYM'): 1, ('PRP', 'RBS'): 1, ('JJR', 'VBZ'): 1, ('SYM', 'SYM'): 1, ('SYM', ')'): 1, ('RBS', ','): 1, ('PRP', \"''\"): 1, (\"''\", 'PRP'): 1, (')', 'WRB'): 1, ('PRP$', 'VBD'): 1, ('RP', 'VBZ'): 1, ('(', '\"'): 1, ('\"', ')'): 1, ('SYM', '$'): 1, ('\"', 'JJR'): 1, ('PRP$', '$'): 1, ('EX', 'RB'): 1, ('VBN', 'VBP'): 1, ('WP$', 'NNPS'): 1, ('PRP$', '('): 1, ('RP', 'RBR'): 1, ('JJR', '('): 1, ('(', 'SYM'): 1, ('VBD', 'VBZ'): 1, ('NNP', 'UH'): 1, ('RBR', 'PRP'): 1, ('RP', 'RP'): 1, ('(', 'CC'): 1, ('CC', ')'): 1, ('WP', 'JJ'): 1, ('FW', 'DT'): 1, ('PRP', 'POS'): 1, ('FW', ':'): 1, ('FW', 'VBZ'): 1, ('VBP', 'PDT'): 1, ('VBG', 'EX'): 1, ('EX', 'IN'): 1, ('WP', 'PRP$'): 1, ('VBP', '('): 1, ('FW', 'NNPS'): 1, ('JJS', '('): 1, ('IN', 'FW'): 1, (')', ')'): 1, ('IN', 'WP$'): 1, (':', 'WRB'): 1, ('WP$', 'NNP'): 1, ('(', '('): 1, ('SYM', ':'): 1, ('EX', 'CC'): 1, ('WRB', '('): 1, ('RB', \"''\"): 1, ('VBN', 'NNPS'): 1, ('RP', 'VBP'): 1, ('VBZ', 'EX'): 1, ('WRB', '\"'): 1, (':', \"''\"): 1, (\"''\", 'VBP'): 1, ('JJR', 'WP'): 1, ('VBZ', 'PDT'): 1, ('RP', 'JJR'): 1, ('NNPS', 'PRP$'): 1, ('NN', 'LS'): 1, ('VB', 'FW'): 1, (\"''\", 'DT'): 1, ('SYM', 'VBG'): 1, ('RP', ':'): 1, ('DT', 'FW'): 1, ('MD', 'VBZ'): 1, ('\"', '\"'): 1, ('RBR', 'PRP$'): 1, ('RB', 'JJS'): 1, ('\"', 'RBR'): 1, ('VBP', 'RBR'): 1, (')', 'PRP'): 1, ('JJS', 'DT'): 1, ('FW', 'TO'): 1, ('VBN', 'POS'): 1, ('VBN', 'PDT'): 1, ('PRP', 'WP'): 1, ('TO', 'WRB'): 1, ('DT', \"''\"): 1, (')', 'WP'): 1, ('VB', 'EX'): 1, ('SYM', 'UH'): 1, ('UH', 'JJ'): 1, ('RBR', 'NNS'): 1, ('VBZ', 'FW'): 1, ('PRP', ')'): 1, ('MD', 'CC'): 1, ('CD', 'JJS'): 1, ('POS', 'MD'): 1, ('RBR', 'NNP'): 1, (',', 'RP'): 1, ('WP', '('): 1, ('NNP', 'JJS'): 1, ('\"', 'JJS'): 1, ('NNP', 'WP$'): 1, ('WP$', 'CD'): 1, ('SYM', 'FW'): 1, ('RBR', 'CC'): 1, ('VBP', 'VBP'): 1, ('JJS', 'VBN'): 1, ('CC', '.'): 1, ('UH', 'UH'): 1, ('IN', 'POS'): 1, ('VB', 'RBS'): 1, ('RBS', 'NNS'): 1, ('JJS', 'CC'): 1, ('WDT', 'TO'): 1, ('POS', 'TO'): 1, ('(', 'VBD'): 1, ('VBD', \"''\"): 1, ('RP', 'VBD'): 1, ('TO', 'WDT'): 1, (')', 'JJR'): 1, ('RB', 'SYM'): 1, ('NNPS', 'SYM'): 1, ('NNS', 'PDT'): 1, ('PRP', 'UH'): 1, ('UH', 'VB'): 1, ('EX', '('): 1, ('NN', 'RBS'): 1, ('RBS', 'DT'): 1, ('VBD', 'PDT'): 1, ('TO', 'RBR'): 1, ('RBR', 'VBP'): 1, ('CC', 'RBR'): 1, ('RBS', 'VBN'): 1, ('VBG', 'RBR'): 1, ('POS', 'DT'): 1, ('NNPS', 'WDT'): 1, ('WP', 'CC'): 1, ('WP', '.'): 1, ('CC', \"''\"): 1, ('WDT', '('): 1, ('SYM', 'PRP'): 1, ('UH', 'NN'): 1, ('NN', 'NN|SYM'): 1, ('NNS', 'JJS'): 1, ('VBZ', 'JJS'): 1, (\"''\", 'WP'): 1, (':', 'JJR'): 1})\n"
     ]
    }
   ],
   "source": [
    "count_labels = Counter()\n",
    "count_words = Counter()\n",
    "count_transitions = Counter()\n",
    "\n",
    "# count the words\n",
    "for sent in train_words:\n",
    "  count_words['<SOS>'] += 1\n",
    "  for word in sent:\n",
    "    # we lowercase all the words, to reduce unique number of tokens\n",
    "    count_words[word] += 1\n",
    "\n",
    "# count the labels\n",
    "for labels in train_pos:\n",
    "  count_labels['<SOS>'] += 1\n",
    "  for label in labels:\n",
    "    count_labels[label] += 1\n",
    "\n",
    "# count the possible transitions for the labels\n",
    "for labels in train_pos:\n",
    "  prev_label = '<SOS>'\n",
    "  for label in labels:\n",
    "    count_transitions[(prev_label, label)] += 1\n",
    "    prev_label = label\n",
    "\n",
    "\n",
    "print('='*10)\n",
    "print('Unique words: {}'.format(len(count_words)))\n",
    "print(f'Number of labels: {len(count_labels)}, labels: {count_labels}')\n",
    "print(f'Number of transitions: {len(count_transitions)}, bigrams: {count_transitions}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lkd0NJqGHeGs"
   },
   "source": [
    "We counted almost everything, we didn't count the conditional pairs of words and labels yet. \n",
    "\n",
    "\n",
    "To avoid zero probabilities, we create an unknown ('\\<UNK\\>') token. We will only use the most common words as features, the counts of the remaining words are moved towards the unknown token. Given we have 21000 unique words, we use the 20.000 most common words.\n",
    "\n",
    "We need to take this into account when counting the word-label pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 403,
     "status": "ok",
     "timestamp": 1635112470797,
     "user": {
      "displayName": "Victor Milewski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8ATR8MBxA3DRxgsWRfETNmC5Ng4t-z_MXil1pw=s64",
      "userId": "00980338011929053842"
     },
     "user_tz": -120
    },
    "id": "VJjfVH7YHems",
    "outputId": "302698fb-f534-492c-ee4a-a9097fbc7ff0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique word-label pairs: 24323, example pairs: ['word=<SOS> tag=<SOS>: 14041', 'word=the tag=DT: 8383', 'word=. tag=.: 7374', 'word=, tag=,: 7290', 'word=of tag=IN: 3815', 'word=in tag=IN: 3601', 'word=to tag=TO: 3424', 'word=a tag=DT: 3184', 'word=and tag=CC: 2872', 'word=( tag=(: 2861']\n"
     ]
    }
   ],
   "source": [
    "count_observation = Counter()\n",
    "\n",
    "# count the conditional pairs between the words and labels\n",
    "vocab_words = dict(count_words.most_common(20000))\n",
    "for sent, labels in zip(train_words, train_pos):\n",
    "  # add a SOS pair at the beginning of every sentence\n",
    "  count_observation[(\"<SOS>\", \"<SOS>\")] += 1\n",
    "  for word, label in zip(sent, labels):\n",
    "    if word in vocab_words.keys(): \n",
    "      count_observation[(word, label)] += 1\n",
    "    else:\n",
    "      count_observation[(\"<UNK>\", label)] += 1\n",
    "print(f'Number of unique word-label pairs: {len(count_observation)}, example pairs: {[f\"word={p[0]} tag={p[1]}: {c}\" for p, c in sorted(count_observation.items(), key=lambda item: item[1],reverse=True)][:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bd2CgetbG0Gt"
   },
   "source": [
    "Now we counted everything, we can create the probabilities matrices and vectors. We must make sure that we always keep the same order for all our words and labels. So we will create a word and label vocab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1635112470798,
     "user": {
      "displayName": "Victor Milewski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8ATR8MBxA3DRxgsWRfETNmC5Ng4t-z_MXil1pw=s64",
      "userId": "00980338011929053842"
     },
     "user_tz": -120
    },
    "id": "9OJ63iDSOn60"
   },
   "outputs": [],
   "source": [
    "word2id_vocab = {k:i for i, (k, c) in enumerate(count_words.most_common(20000))}\n",
    "word2id_vocab['<UNK>'] = 20000\n",
    "label2id_vocab = {k:i for i, (k, c) in enumerate(count_labels.most_common())}\n",
    "id2label_vocab = {i:k for k, i in label2id_vocab.items()}\n",
    "id2word_vocab = {v: k for k, v in word2id_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1635112470799,
     "user": {
      "displayName": "Victor Milewski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8ATR8MBxA3DRxgsWRfETNmC5Ng4t-z_MXil1pw=s64",
      "userId": "00980338011929053842"
     },
     "user_tz": -120
    },
    "id": "nsKZrZmGzw2D"
   },
   "outputs": [],
   "source": [
    "# # first we count the total\n",
    "# total_words = np.sum(list(count_words.values()))\n",
    "\n",
    "# # now we create the probabilities\n",
    "# prob_words_vector = np.zeros(len(word2id_vocab))\n",
    "# used_count = 0  # we keep track of the used counts, so we can use the remaining count for the unknown token. \n",
    "# for word, id in word2id_vocab.items():\n",
    "#   if word != '<UNK>':\n",
    "#     prob_words_vector[id] = count_words[word]/total_words\n",
    "#     used_count += count_words[word]\n",
    "# # now we add the probability for the unknown token\n",
    "# prob_words_vector[word2id_vocab['<UNK>']] = (total_words-used_count)/total_words\n",
    "# print(np.sum(prob_words_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRe8D26BJQ63"
   },
   "source": [
    "Next let us compute the probabilities for the transitions. Since we might have missed a transition, we make use of laplace smoothing for the transition probabilities.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1635112470801,
     "user": {
      "displayName": "Victor Milewski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8ATR8MBxA3DRxgsWRfETNmC5Ng4t-z_MXil1pw=s64",
      "userId": "00980338011929053842"
     },
     "user_tz": -120
    },
    "id": "YHyKEa9dJPuW",
    "outputId": "41c1d5b4-a00f-43a7-a1e1-a5e7bf5b1aa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# # first we count the total\n",
    "# total = np.sum(list(count_labels.values()))\n",
    "\n",
    "# # now we create the probabilities\n",
    "# prob_labels_vector = np.zeros(len(label2id_vocab))\n",
    "# for label, id in label2id_vocab.items():\n",
    "#   prob_labels_vector[id] = count_labels[label]/total\n",
    "\n",
    "## NOW THE TRANSITIONS\n",
    "conditional_totals = Counter()\n",
    "for bigram, count in count_transitions.items():\n",
    "  conditional_totals[bigram[0]] += count\n",
    "smoothing_constant = len(label2id_vocab)\n",
    "\n",
    "# now we create the probability matrix\n",
    "prob_transition_matrix = np.zeros([len(label2id_vocab), len(label2id_vocab)])\n",
    "for label_i, id_i in label2id_vocab.items():\n",
    "  denominator = conditional_totals[label_i] + smoothing_constant\n",
    "  for label_j, id_j in label2id_vocab.items():\n",
    "    if (label_i, label_j) in count_transitions:\n",
    "      numerator = count_transitions[(label_i, label_j)] + 1\n",
    "    else:\n",
    "      numerator = 1\n",
    "    prob_transition_matrix[id_i, id_j] = numerator/denominator\n",
    "# print(np.sum(prob_labels_vector))\n",
    "print(np.sum(prob_transition_matrix, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1635112470806,
     "user": {
      "displayName": "Victor Milewski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8ATR8MBxA3DRxgsWRfETNmC5Ng4t-z_MXil1pw=s64",
      "userId": "00980338011929053842"
     },
     "user_tz": -120
    },
    "id": "Y2BdaP_DBKOE",
    "outputId": "719e5ef6-6f18-4ce3-c91e-cd0cb56ea7f2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NNP</th>\n",
       "      <th>NN</th>\n",
       "      <th>CD</th>\n",
       "      <th>IN</th>\n",
       "      <th>&lt;SOS&gt;</th>\n",
       "      <th>DT</th>\n",
       "      <th>JJ</th>\n",
       "      <th>NNS</th>\n",
       "      <th>VBD</th>\n",
       "      <th>.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NNP</th>\n",
       "      <td>0.330218</td>\n",
       "      <td>0.047199</td>\n",
       "      <td>0.118755</td>\n",
       "      <td>0.052183</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.003323</td>\n",
       "      <td>0.011570</td>\n",
       "      <td>0.017058</td>\n",
       "      <td>0.057850</td>\n",
       "      <td>0.050314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN</th>\n",
       "      <td>0.040002</td>\n",
       "      <td>0.121779</td>\n",
       "      <td>0.028217</td>\n",
       "      <td>0.271479</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.007434</td>\n",
       "      <td>0.011743</td>\n",
       "      <td>0.052167</td>\n",
       "      <td>0.064459</td>\n",
       "      <td>0.094703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CD</th>\n",
       "      <td>0.088875</td>\n",
       "      <td>0.065162</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.002352</td>\n",
       "      <td>0.027718</td>\n",
       "      <td>0.109282</td>\n",
       "      <td>0.003814</td>\n",
       "      <td>0.033630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IN</th>\n",
       "      <td>0.237118</td>\n",
       "      <td>0.093871</td>\n",
       "      <td>0.072673</td>\n",
       "      <td>0.017158</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.314828</td>\n",
       "      <td>0.077238</td>\n",
       "      <td>0.040298</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.000840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;SOS&gt;</th>\n",
       "      <td>0.384894</td>\n",
       "      <td>0.078015</td>\n",
       "      <td>0.076099</td>\n",
       "      <td>0.038049</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.092355</td>\n",
       "      <td>0.055086</td>\n",
       "      <td>0.058210</td>\n",
       "      <td>0.003549</td>\n",
       "      <td>0.000426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DT</th>\n",
       "      <td>0.122002</td>\n",
       "      <td>0.434841</td>\n",
       "      <td>0.030445</td>\n",
       "      <td>0.007129</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>0.269919</td>\n",
       "      <td>0.062375</td>\n",
       "      <td>0.003861</td>\n",
       "      <td>0.001411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JJ</th>\n",
       "      <td>0.065408</td>\n",
       "      <td>0.461767</td>\n",
       "      <td>0.028664</td>\n",
       "      <td>0.050608</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.004933</td>\n",
       "      <td>0.091265</td>\n",
       "      <td>0.159990</td>\n",
       "      <td>0.005699</td>\n",
       "      <td>0.016926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNS</th>\n",
       "      <td>0.013941</td>\n",
       "      <td>0.021479</td>\n",
       "      <td>0.018381</td>\n",
       "      <td>0.279740</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.006506</td>\n",
       "      <td>0.013527</td>\n",
       "      <td>0.008777</td>\n",
       "      <td>0.103779</td>\n",
       "      <td>0.111524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBD</th>\n",
       "      <td>0.066803</td>\n",
       "      <td>0.031599</td>\n",
       "      <td>0.046137</td>\n",
       "      <td>0.145741</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.162441</td>\n",
       "      <td>0.047699</td>\n",
       "      <td>0.024150</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>0.080380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.002188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            NNP        NN        CD  ...       NNS       VBD         .\n",
       "NNP    0.330218  0.047199  0.118755  ...  0.017058  0.057850  0.050314\n",
       "NN     0.040002  0.121779  0.028217  ...  0.052167  0.064459  0.094703\n",
       "CD     0.088875  0.065162  0.461538  ...  0.109282  0.003814  0.033630\n",
       "IN     0.237118  0.093871  0.072673  ...  0.040298  0.000682  0.000840\n",
       "<SOS>  0.384894  0.078015  0.076099  ...  0.058210  0.003549  0.000426\n",
       "DT     0.122002  0.434841  0.030445  ...  0.062375  0.003861  0.001411\n",
       "JJ     0.065408  0.461767  0.028664  ...  0.159990  0.005699  0.016926\n",
       "NNS    0.013941  0.021479  0.018381  ...  0.008777  0.103779  0.111524\n",
       "VBD    0.066803  0.031599  0.046137  ...  0.024150  0.002163  0.080380\n",
       ".      0.002188  0.002188  0.002188  ...  0.002188  0.002188  0.002188\n",
       "\n",
       "[10 rows x 10 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the matrix to a df for better readability\n",
    "tags_df = pd.DataFrame(prob_transition_matrix, \n",
    "                       columns = label2id_vocab.keys(), index=label2id_vocab.keys())\n",
    "tags_df.iloc[:10, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFPGK7IWMyUo"
   },
   "source": [
    "And finally we compute the state observation probabilities, for a word given a label. Even though we already have unknown labels for the words, we will still use laplace smoothing here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 850,
     "status": "ok",
     "timestamp": 1635112471637,
     "user": {
      "displayName": "Victor Milewski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8ATR8MBxA3DRxgsWRfETNmC5Ng4t-z_MXil1pw=s64",
      "userId": "00980338011929053842"
     },
     "user_tz": -120
    },
    "id": "NhQc9sy8AznY",
    "outputId": "f2d18fcb-499e-466f-a91f-8bc93993304c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Most likely word per label\n",
      "['LABEL: NNP, WORD: u.s., LIKELIHOOD:0.01', 'LABEL: NN, WORD: year, LIKELIHOOD:0.01', 'LABEL: CD, WORD: 1, LIKELIHOOD:0.04', 'LABEL: IN, WORD: of, LIKELIHOOD:0.10', 'LABEL: <SOS>, WORD: <SOS>, LIKELIHOOD:0.41', 'LABEL: DT, WORD: the, LIKELIHOOD:0.25', 'LABEL: JJ, WORD: first, LIKELIHOOD:0.01', 'LABEL: NNS, WORD: results, LIKELIHOOD:0.01', 'LABEL: VBD, WORD: said, LIKELIHOOD:0.06', 'LABEL: ., WORD: ., LIKELIHOOD:0.27', 'LABEL: ,, WORD: ,, LIKELIHOOD:0.27', 'LABEL: VB, WORD: be, LIKELIHOOD:0.02', 'LABEL: VBN, WORD: been, LIKELIHOOD:0.02', 'LABEL: RB, WORD: not, LIKELIHOOD:0.02', 'LABEL: CC, WORD: and, LIKELIHOOD:0.12', 'LABEL: TO, WORD: to, LIKELIHOOD:0.15', 'LABEL: PRP, WORD: he, LIKELIHOOD:0.03', 'LABEL: (, WORD: (, LIKELIHOOD:0.13', 'LABEL: ), WORD: ), LIKELIHOOD:0.13', 'LABEL: VBG, WORD: being, LIKELIHOOD:0.00', 'LABEL: VBZ, WORD: is, LIKELIHOOD:0.03', 'LABEL: :, WORD: -, LIKELIHOOD:0.06', 'LABEL: \", WORD: \", LIKELIHOOD:0.10', \"LABEL: POS, WORD: 's, LIKELIHOOD:0.07\", 'LABEL: PRP$, WORD: his, LIKELIHOOD:0.03', 'LABEL: VBP, WORD: are, LIKELIHOOD:0.02', 'LABEL: MD, WORD: will, LIKELIHOOD:0.02', 'LABEL: NNPS, WORD: palestinians, LIKELIHOOD:0.00', 'LABEL: WP, WORD: who, LIKELIHOOD:0.02', 'LABEL: RP, WORD: up, LIKELIHOOD:0.01', 'LABEL: WDT, WORD: which, LIKELIHOOD:0.02', 'LABEL: SYM, WORD: /, LIKELIHOOD:0.01', 'LABEL: $, WORD: $, LIKELIHOOD:0.02', 'LABEL: WRB, WORD: when, LIKELIHOOD:0.01', 'LABEL: JJR, WORD: more, LIKELIHOOD:0.01', 'LABEL: JJS, WORD: most, LIKELIHOOD:0.00', 'LABEL: FW, WORD: v, LIKELIHOOD:0.00', 'LABEL: RBR, WORD: earlier, LIKELIHOOD:0.00', 'LABEL: EX, WORD: there, LIKELIHOOD:0.01', 'LABEL: RBS, WORD: most, LIKELIHOOD:0.00', \"LABEL: '', WORD: ', LIKELIHOOD:0.00\", 'LABEL: PDT, WORD: all, LIKELIHOOD:0.00', 'LABEL: UH, WORD: o, LIKELIHOOD:0.00', 'LABEL: WP$, WORD: whose, LIKELIHOOD:0.00', 'LABEL: LS, WORD: 212, LIKELIHOOD:0.00', 'LABEL: NN|SYM, WORD: tvm, LIKELIHOOD:0.00']\n"
     ]
    }
   ],
   "source": [
    "conditional_totals = Counter()\n",
    "for (word,label), count in count_observation.items():\n",
    "  conditional_totals[label] += count\n",
    "# we define a smoothing constant for the laplace smoothing. This is equal to the number of possible observations\n",
    "smoothing_constant = len(word2id_vocab)\n",
    "\n",
    "# now we create the probability matrix\n",
    "prob_observation_matrix = np.zeros([len(label2id_vocab), len(word2id_vocab)])\n",
    "for label, label_id in label2id_vocab.items():\n",
    "  denominator = conditional_totals[label] + smoothing_constant\n",
    "  for word, word_id in word2id_vocab.items():\n",
    "    if (word, label) in count_observation:\n",
    "      numerator = count_observation[(word, label)] + 1\n",
    "    else:\n",
    "      numerator = 1\n",
    "    prob_observation_matrix[label_id, word_id] = numerator/denominator\n",
    "print(np.sum(prob_observation_matrix, 1))\n",
    "print('Most likely word per label')\n",
    "print([f'LABEL: {label}, WORD: {id2word_vocab[max_id]}, LIKELIHOOD:{prob:.2f}' for label, max_id,prob in zip(id2label_vocab.values(),np.argmax(prob_observation_matrix, 1),np.max(prob_observation_matrix, 1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUlrG-OVU9yk"
   },
   "source": [
    "$P(W_t=v_i|L_t=l_j)$ =``prob_observation_matrix[i][j]``\n",
    "\n",
    "$P(L_t=l_i|L_{t-1}=l_j)$ =``prob_transition_matrix[i][j]`` \n",
    "\n",
    "where $W_x$, $L_x$ are random variables corresponding to a word resp. word label in our dataset at location $x$.\n",
    "$v_i$ is the $i$th word in the word vocabulary, and $l_j$ the $j$th label in the label vocabulary.\n",
    "<!-- $P(lvar_{0:T}=lval_{0:T}|wvar_{0:T}=wval_{0:T}) = \\prod_{t=0}^{T-1}{P(lvar_t=lval_t)|lvar_{0:t}=lval_{0:t}|wvar_{0:T}=wval_{0:T}}$\n",
    "$P(l_t=s|w_t=v,l_{t-1}=s') = \\frac{P()}{P()}$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmbwG74ZMVF4"
   },
   "source": [
    "Now we have the needed matrices correctly created (they all sum to a probability of 1), we can use this to predict a sequence of labels for new sentences in our dev-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1635113513249,
     "user": {
      "displayName": "Victor Milewski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8ATR8MBxA3DRxgsWRfETNmC5Ng4t-z_MXil1pw=s64",
      "userId": "00980338011929053842"
     },
     "user_tz": -120
    },
    "id": "_r_s7N7Nn32O",
    "outputId": "db83d31b-5d6b-4fd0-99ab-34c199b80310"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                 True POS tag    Pred. POS tag  \n",
      "\"                    \"               \"              \n",
      "lebed                NN              NNP            \n",
      "is                   VBZ             VBZ            \n",
      "now                  RB              RB             \n",
      "in                   IN              IN             \n",
      "chechnya             NNP             NNP            \n",
      "solving              VBG             NNP            \n",
      "some                 DT              DT             \n",
      "problems             NNS             NNS            \n",
      ",                    ,               ,              \n",
      "\"                    \"               \"              \n",
      "interfax             NN              NNP            \n",
      "quoted               VBN             VBD            \n",
      "chernomyrdin         NNP             NNP            \n",
      "as                   IN              IN             \n",
      "saying               VBG             VBG            \n",
      ".                    .               .              \n",
      "\"                    \"               \"              \n",
      "pred probabilty 7.203288184421062e-60\n"
     ]
    }
   ],
   "source": [
    "def greedy_sentence_annotation(sentence):\n",
    "  # we initialize our sequence with the SOS token\n",
    "  sos_lab_id = label2id_vocab[\"<SOS>\"]\n",
    "  sos_word_id = word2id_vocab[\"<SOS>\"]\n",
    "  sentence_probability = prob_observation_matrix[sos_lab_id, sos_word_id]\n",
    "  prev_id = sos_lab_id\n",
    "  prediction_labels = []\n",
    "  for word in sentence:\n",
    "    if word in word2id_vocab:\n",
    "      word_id = word2id_vocab[word]\n",
    "    else:\n",
    "      word_id = word2id_vocab[\"<UNK>\"]\n",
    "    probs_possible_label = sentence_probability.copy()\n",
    "    probs_possible_label *= prob_observation_matrix[:, word_id]\n",
    "    probs_possible_label *= prob_transition_matrix[prev_id, :]\n",
    "    # This is where we are 'greedy': we just continue working with the best option at each timestep\n",
    "    prev_id = np.argmax(probs_possible_label) \n",
    "    prediction_labels.append(id2label_vocab[prev_id])\n",
    "    sentence_probability = np.max(probs_possible_label)\n",
    "  return sentence_probability, prediction_labels\n",
    "\n",
    "def test_one_sentence(index):\n",
    "  prob, pred_labels = greedy_sentence_annotation(dev_words[index])\n",
    "  col_widths_formatting = '{:<20s} {:<15s} {:<15s}'\n",
    "  print( col_widths_formatting.format(\"Word\",\"True POS tag\",\"Pred. POS tag\") )\n",
    "  for w,t,p in zip(dev_words[index],dev_pos[index],pred_labels):\n",
    "    print( col_widths_formatting.format(w,t,p) )\n",
    "  # print('\\t'.join(dev_words[index]))\n",
    "  # print('\\t\\t'.join(pred_labels))\n",
    "  # print('\\t\\t'.join(dev_pos[index]))\n",
    "  print(\"pred probabilty\",prob)\n",
    "\n",
    "test_one_sentence(830)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "executionInfo": {
     "elapsed": 284,
     "status": "ok",
     "timestamp": 1635113520863,
     "user": {
      "displayName": "Victor Milewski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8ATR8MBxA3DRxgsWRfETNmC5Ng4t-z_MXil1pw=s64",
      "userId": "00980338011929053842"
     },
     "user_tz": -120
    },
    "id": "dcMBPKVdNXel"
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(linewidth=np.inf, precision=2) # To not have matrices wrap around when printing\n",
    "def viterbi(sentence):\n",
    "  T = len(sentence)+1\n",
    "  N = len(label2id_vocab)\n",
    "  viterbi_matrix = np.zeros((N,T))\n",
    "  backpointer_matrix = np.zeros((N,T))\n",
    "  sos_lab_id = label2id_vocab[\"<SOS>\"]\n",
    "  sos_word_id = word2id_vocab[\"<SOS>\"]\n",
    "  start_prob = prob_observation_matrix[sos_lab_id, sos_word_id]\n",
    "  # Probability of every label at the start is 0, except for the <SOS> tag. \n",
    "  for s in range(N):\n",
    "    viterbi_matrix[s,0] = 0\n",
    "    backpointer_matrix[s,0] = sos_lab_id\n",
    "  viterbi_matrix[sos_lab_id, 0] = start_prob\n",
    "\n",
    "  for t_step in range(1,T):\n",
    "    if sentence[t_step-1] in word2id_vocab:\n",
    "      word_id = word2id_vocab[sentence[t_step-1]]\n",
    "    else:\n",
    "      word_id = word2id_vocab[\"<UNK>\"]\n",
    "    for s in range(N):\n",
    "      state_prob = viterbi_matrix[:, t_step-1].copy()           # probability of sequence till now\n",
    "      state_prob *= prob_transition_matrix[:, s]         # probability for transition from every previous label to label s\n",
    "      state_prob *= prob_observation_matrix[s, word_id]  # probability observation\n",
    "      viterbi_matrix[s, t_step] = np.max(state_prob)\n",
    "      backpointer_matrix[s, t_step] = np.argmax(state_prob)\n",
    "\n",
    "  # now we simply select the best path\n",
    "  best_path_prob = np.max(viterbi_matrix[:, -1])\n",
    "  best_path_pointer = int(np.argmax(viterbi_matrix[:, -1]))\n",
    "  # given the path, we track back to create the entire label sequence\n",
    "  best_path = [best_path_pointer]\n",
    "  for t_step in range(T-1, 1, -1):\n",
    "    best_path.append(int(backpointer_matrix[best_path[-1], t_step]))\n",
    "  best_path = [id2label_vocab[i] for i in best_path[::-1]]\n",
    "  return best_path_prob, best_path\n",
    "\n",
    "\n",
    "def test_one_sentence_viterbi(index):\n",
    "  prob, pred_labels = viterbi(dev_words[index])\n",
    "  col_widths_formatting = '{:<20s} {:<15s} {:<15s}'\n",
    "  print( col_widths_formatting.format(\"Word\",\"True POS tag\",\"Pred. POS tag\") )\n",
    "  for w,t,p in zip(dev_words[index],dev_pos[index],pred_labels):\n",
    "    print( col_widths_formatting.format(w,t,p) )\n",
    "  print(\"pred probabilty\",prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1635113521736,
     "user": {
      "displayName": "Victor Milewski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8ATR8MBxA3DRxgsWRfETNmC5Ng4t-z_MXil1pw=s64",
      "userId": "00980338011929053842"
     },
     "user_tz": -120
    },
    "id": "VBUPPejs-gQP",
    "outputId": "7dc11daa-81b6-44c3-98f3-ceef161a858b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                 True POS tag    Pred. POS tag  \n",
      "\"                    \"               \"              \n",
      "lebed                NN              NNP            \n",
      "is                   VBZ             VBZ            \n",
      "now                  RB              RB             \n",
      "in                   IN              IN             \n",
      "chechnya             NNP             NNP            \n",
      "solving              VBG             VBD            \n",
      "some                 DT              DT             \n",
      "problems             NNS             NNS            \n",
      ",                    ,               ,              \n",
      "\"                    \"               \"              \n",
      "interfax             NN              PRP            \n",
      "quoted               VBN             VBD            \n",
      "chernomyrdin         NNP             VBN            \n",
      "as                   IN              IN             \n",
      "saying               VBG             VBG            \n",
      ".                    .               .              \n",
      "\"                    \"               \"              \n",
      "pred probabilty 1.8388642619654403e-58\n"
     ]
    }
   ],
   "source": [
    "test_one_sentence_viterbi(830)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoMKSII2RaNH"
   },
   "source": [
    "## Evaluation\n",
    "For the evaluation we will make use of the F1 metric. We simply use a pre-implemented function from sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33339,
     "status": "ok",
     "timestamp": 1635113558268,
     "user": {
      "displayName": "Victor Milewski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8ATR8MBxA3DRxgsWRfETNmC5Ng4t-z_MXil1pw=s64",
      "userId": "00980338011929053842"
     },
     "user_tz": -120
    },
    "id": "r5xZ2pR2_Lxx",
    "outputId": "1d6c26df-d114-43f5-abcf-a6bfe48bdf88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score Greedy HMM:0.6256. total runtime: 0.8335\n",
      "F1 Score viterbi HMM:0.6687. total runtime: 31.7705\n"
     ]
    }
   ],
   "source": [
    "all_true_labels = []\n",
    "all_greedy_pred = []\n",
    "all_viterbi_pred = []\n",
    "greedy_times = []\n",
    "viterbi_times = []\n",
    "\n",
    "for sent, true_labels in zip(dev_words, dev_pos):\n",
    "  start = time.time()\n",
    "  _, greedy_pred = greedy_sentence_annotation(sent)\n",
    "  stop = time.time()\n",
    "  greedy_times.append(stop-start)\n",
    "  start=time.time()\n",
    "  _, vitebi_pred = viterbi(sent)\n",
    "  stop=time.time()\n",
    "  viterbi_times.append(stop-start)\n",
    "  all_true_labels += true_labels\n",
    "  all_greedy_pred += greedy_pred\n",
    "  all_viterbi_pred += vitebi_pred\n",
    "\n",
    "f1_greedy = f1_score(all_true_labels, all_greedy_pred, average='macro')\n",
    "f1_viterbi = f1_score(all_true_labels, all_viterbi_pred, average='macro')\n",
    "\n",
    "print(\"F1 Score Greedy HMM:{}. total runtime: {}\".format(np.round(f1_greedy,4),np.round(np.sum(greedy_times),4)))\n",
    "print(\"F1 Score viterbi HMM:{}. total runtime: {}\".format(np.round(f1_viterbi,4),np.round(np.sum(viterbi_times),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 315,
     "status": "aborted",
     "timestamp": 1635112471941,
     "user": {
      "displayName": "Victor Milewski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8ATR8MBxA3DRxgsWRfETNmC5Ng4t-z_MXil1pw=s64",
      "userId": "00980338011929053842"
     },
     "user_tz": -120
    },
    "id": "-ZK8u1pcKidI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "exercise_1-POS_NER_ER-coding-Solutions.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
